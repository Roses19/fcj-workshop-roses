[{"uri":"https://roses19.github.io/fcj-workshop-roses/3-blogstranslated/3.1-blog1/","title":"Build and manage your modern data stack using dbt and AWS Glue through dbt-glue, the new trusted dbt adapter","tags":[],"description":"","content":"Original blog:: AWS Big Data Blog – dbt \u0026amp; AWS Glue\nIntroduction dbt is an open-source, SQL-first tool that allows you to write reusable and extensible data transformations using SQL and Python. dbt focuses on the transform layer in the ELT/ETL workflow on data warehouses and databases through corresponding adapters to enable extraction and loading of data.\nThis allows data engineers, data scientists, and analytics engineers to define business logic using SQL SELECT statements and eliminates the need to manually write DML and DDL statements. dbt also enables fast and collaborative analytics code deployment following software development best practices such as modularity, portability, CI/CD, and documentation.\ndbt is primarily used with data warehouses (such as Amazon Redshift), where customers want to separate transformation logic from storage and compute engines.\nTo meet the demand for cloud data lakes, AWS introduced the dbt-glue adapter in 2022 — an AWS Glue adapter for open-source dbt that is battle-tested, enabling data engineers to use dbt across both data lakes and data warehouses, paying only for the compute they need.\nAdapter dbt-glue extends dbt access to data lakes and allows running data transformation workloads on AWS Glue’s serverless Spark with ease. AWS continues to invest in dbt-glue to support additional requirements.\nToday, we are pleased to announce that the dbt-glue adapter is now a trusted adapter based on our strategic collaboration with dbt Labs. Trusted adapters are adapters not maintained by dbt Labs, but adaptors that that dbt Lab is comfortable recommending to users for use in production.\nThe key capabilities of the dbt-glue adapter are as follows:\nRuns SQL as Spark SQL on AWS Glue interactive sessions. Manages table definitions on the Amazon SageMaker Lakehouse Catalog with storage on Amazon S3. Supports open table formats such as Apache Hudi, Delta Lake, and Apache Iceberg Supports AWS Lake Formation permissions for fine-grained access control In addition to those capabilities, the dbt-glue adapter is designed to optimize resource utilization with several techniques on top of AWS Glue interactive sessions.\nThis post demonstrates how the dbt-glue adapter helps your workload, and how you can build a modern data stack using dbt and AWS Glue using the dbt-glue adapter.\nCommon use cases 1. A central analytics team at a large corporation\nA central analytics team needs to track operational performance. They ingest application logs into Parquet tables in the raw layer of an S3-based data lake. They also extract structured data from operational systems.\nFor example, organizational schema and cost data for different components are stored in the raw layer using Iceberg to preserve the original schema for multiple data access patterns.\nThe team uses dbt-glue to build gold models optimized for BI — combining technical logs with billing data and organizing metrics by business unit. Iceberg supports warehouse-style modeling for high-performance BI analytics. Combining Iceberg and dbt-glue enables building data models that are ready for consumption.\n2. Building GDPR-compliant data products\nAn analytics team at a European company operates a data lake on S3 and needs to build new data products to enrich healthcare data, which requires GDPR compliance such as the right to be forgotten and data deletion. They use Iceberg to meet these requirements and dbt to model data on the data lake because dbt-glue supports Iceberg and simplifies the use of this storage format.\nHow dbt and dbt-glue work The following are key dbt features::\nProject – A dbt project enforces a top-level structure on the staging, models, permissions, and adapters. A project can be checked into a GitHub repo for version control. SQL – dbt relies on SQL select statements for defining data transformation logic. Instead of raw SQL, dbt offers templatized SQL (using Jinja) that allows code modularity. Instead of having to copy/paste SQL in multiple places, data engineers can define modular transforms and call those from other places within the project. Having a modular pipeline helps data engineers collaborate on the same project. Models – dbt models are primarily written as a SELECT statement and saved as a .sql file. Data engineers define dbt models for their data representations. Materializations – Materializations are strategies for persisting dbt models in a warehouse. There are five types of materializations built into dbt: table, view, incremental, ephemeral, and materialized view. Data lineage – dbt tracks data lineage, allowing you to understand the origin of data and how it flows through different transformations. dbt also supports impact analysis, which helps identify the downstream effects of changes. The high-level data flow is as follows:\nData engineers ingest data from data sources to raw tables and define table definitions for the raw tables.\nData engineers write dbt models with templatized SQL.\nThe dbt adapter converts dbt models to SQL statements compatible in a data warehouse.\nThe data warehouse runs the SQL statements to create intermediate tables or final tables, views, or materialized views.\nThe following diagram illustrates the architecture. dbt-glue works with the following steps:\nThe dbt-glue adapter converts dbt models to SQL statements compatible in Spark SQL. 2.AWS Glue interactive sessions run the SQL statements to create intermediate tables or final tables, views, or materialized views.\ndbt-glue supports: csv, parquet, hudi, delta, iceberg.\nOn the dbt-glue adapter, table or incremental are commonly used for materializations at the destination. There are three strategies for incremental materialization. The merge strategy requires hudi, delta, or iceberg. With the other two strategies, append and insert_overwrite, you can use any of the formats above.\nThe following diagram illustrates this architecture. Example use case In this post, we use the data from the New York City Taxi Records dataset. This dataset is available in the Open Data on AWS (RODA)which is a repository containing public datasets from AWS resources. The raw Parquet table records in this dataset stores trip records.\nThe objective is to create the following three tables, which contain metrics based on the raw table: silver_avg_metrics – Basic metrics based on NYC Taxi Open Data for the year 2016 gold_passengers_metrics – Metrics per passenger based on the silver metrics table gold_cost_metrics – Metrics per cost based on the silver metrics table The final goal is to create two well-designed gold tables that store already aggregated results in Iceberg format for ad hoc queries through Amazon Athena.\nPrerequisites Prepare an IAM Role with permissions to run Glue interactive session and dbt-glue\nCreate a Glue database and tables for the Taxi dataset\nCreate an S3 bucket for output data\nConfigure Athena to explore the data\nUse CloudFormation to deploy all infrastructure quickly\nWith these prerequisites, we simulate the situation that data engineers have already ingested data from data sources to raw tables, and defined table definitions for the raw tables.\nFor ease of use, we prepared a CloudFormation template. This template deploys all the required infrastructure. To create these resources, choose Launch Stack in the us-east-1 RegionRegion.\nDetailed steps: 1. Install dbt, the dbt CLI : $ pip3 install --no-cache-dir dbt-core 2. Install the dbt-glue adapter :$ pip3 install --no-cache-dir dbt-glue 3. Initialize a dbt project with with $ dbt init and enter the name dbt_glue_demo. Choose the gluedatabase. A new empty project will be created:\n$ cd dbt_glue_demo $ tree . . ├── README.md ├── analyses ├── dbt_project.yml ├── macros ├── models │ └── example │ ├── my_first_dbt_model.sql │ ├── my_second_dbt_model.sql │ └── schema.yml ├── seeds ├── snapshots └── tests 4. Create a source: In models/create the file source_tables.yml:\nversion: 2 sources: - name: data_source schema: nyctaxi tables: - name: records This source definition corresponds to the AWS Glue table nyctaxi.records, which we created in the CloudFormation stack.\n5. Create models\nIn this step, we create a dbt model that represents the average values for trip duration, passenger count, trip distance, and total amount of charges. Complete the following steps:\nCreate the silver folder under models, then create the file silver_avg_metrics.sql with the following content: WITH source_avg as ( SELECT avg((CAST(dropoff_datetime as LONG) - CAST(pickup_datetime as LONG))/60) as avg_duration , avg(passenger_count) as avg_passenger_count , avg(trip_distance) as avg_trip_distance , avg(total_amount) as avg_total_amount , year , month , type FROM {{ source(\u0026#39;data_source\u0026#39;, \u0026#39;records\u0026#39;) }} WHERE year = \u0026#34;2016\u0026#34; AND dropoff_datetime is not null GROUP BY year, month, type ) SELECT * FROM source_avg Create the file models/silver/schema.yml with the following contents: version: 2 models: - name: silver_avg_metrics description: This table has basic metrics based on NYC Taxi Open Data for the year 2016 columns: - name: avg_duration description: The average duration of a NYC Taxi trip - name: avg_passenger_count description: The average number of passenger per NYC Taxi trip - name: avg_trip_distance description: The average NYC Taxi trip distance - name: avg_total_amount description: The avarage amount of a NYC Taxi trip - name: year description: The year of the NYC Taxi trip - name: month description: The month of the NYC Taxi trip - name: type description: The type of the NYC Taxi Create a gold folder under models, and inside it create the file gold_cost_metrics.sql: {{ config( materialized=\u0026#39;incremental\u0026#39;, incremental_strategy=\u0026#39;merge\u0026#39;, unique_key=[\u0026#34;year\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;type\u0026#34;], file_format=\u0026#39;iceberg\u0026#39;, iceberg_expire_snapshots=\u0026#39;False\u0026#39;, table_properties={\u0026#39;format-version\u0026#39;: \u0026#39;2\u0026#39;} ) }} SELECT (avg_total_amount/avg_trip_distance) as avg_cost_per_distance , (avg_total_amount/avg_duration) as avg_cost_per_minute , year , month , type FROM {{ ref(\u0026#39;silver_avg_metrics\u0026#39;) }} Create the file models/gold/gold_passengers_metrics.sql with the following contents: {{ config( materialized=\u0026#39;incremental\u0026#39;, incremental_strategy=\u0026#39;merge\u0026#39;, unique_key=[\u0026#34;year\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;type\u0026#34;], file_format=\u0026#39;iceberg\u0026#39;, iceberg_expire_snapshots=\u0026#39;False\u0026#39;, table_properties={\u0026#39;format-version\u0026#39;: \u0026#39;2\u0026#39;} ) }} SELECT (avg_total_amount/avg_passenger_count) as avg_cost_per_passenger , (avg_duration/avg_passenger_count) as avg_duration_per_passenger , (avg_trip_distance/avg_passenger_count) as avg_trip_distance_per_passenger , year , month , type FROM {{ ref(\u0026#39;silver_avg_metrics\u0026#39;) }} Create the file models/gold/schema.yml with the following contents: version: 2 models: - name: gold_cost_metrics description: This table has metrics per cost based on NYC Taxi Open Data columns: - name: avg_cost_per_distance description: The average cost per distance of a NYC Taxi trip - name: avg_cost_per_minute description: The average cost per minute of a NYC Taxi trip - name: year description: The year of the NYC Taxi trip - name: month description: The month of the NYC Taxi trip - name: type description: The type of the NYC Taxi - name: gold_passengers_metrics description: This table has metrics per passenger based on NYC Taxi Open Data columns: - name: avg_cost_per_passenger description: The average cost per passenger for a NYC Taxi trip - name: avg_duration_per_passenger description: The average number of passenger per NYC Taxi trip - name: avg_trip_distance_per_passenger description: The average NYC Taxi trip distance - name: year description: The year of the NYC Taxi trip - name: month description: The month of the NYC Taxi trip - name: type description: The type of the NYC Taxi Remove the models/example/ folder, because it’s just an example created in the dbt init command. 6. Configure the dbt project\n-dbt_project.yml is a key configuration file for dbt projects. It contains the following code:\nmodels: dbt_glue_demo: # Config indicated by + and applies to all files under models/example/ example: +materialized: view We configure dbt_project.yml to replace the preceding code with the following: models: dbt_glue_demo: silver: +materialized: table This is because that we want to materialize the models under silver as Parquet tables. 7. Configure a dbt profile\ndbt profile is a configuration that specifies how to connect to a particular database. The profiles are defined in the profiles.yml file within a dbt project.\nComplete the following steps to configure a dbt profile:\nCreate the profiles directory.\nCreate the file profiles/profiles.yml with the following contents:\ndbt_glue_demo: target: dev outputs: dev: type: glue query-comment: demo-nyctaxi role_arn: \u0026#34;{{ env_var(\u0026#39;DBT_ROLE_ARN\u0026#39;) }}\u0026#34; region: us-east-1 workers: 5 worker_type: G.1X schema: \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34; database: \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34; session_provisioning_timeout_in_seconds: 120 location: \u0026#34;{{ env_var(\u0026#39;DBT_S3_LOCATION\u0026#39;) }}\u0026#34; Create the profiles/iceberg/ directory..\nCreate the file profiles/iceberg/profiles.yml with the following contents:\ndbt_glue_demo: target: dev outputs: dev: type: glue query-comment: demo-nyctaxi role_arn: \u0026#34;{{ env_var(\u0026#39;DBT_ROLE_ARN\u0026#39;) }}\u0026#34; region: us-east-1 workers: 5 worker_type: G.1X schema: \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34; database: \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34; session_provisioning_timeout_in_seconds: 120 location: \u0026#34;{{ env_var(\u0026#39;DBT_S3_LOCATION\u0026#39;) }}\u0026#34; datalake_formats: \u0026#34;iceberg\u0026#34; conf: spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql catalog.glue_catalog.warehouse={{ env_var(\u0026#39;DBT_S3_LOCATION\u0026#39;) }}warehouse/ --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions The last two lines are added for setting Iceberg configurations on AWS Glue interactive sessions.\n8. Run the dbt project\nRun the dbt project\nTo run the project dbt, you should be in the project folder: ``$ cd dbt_glue_demo``` The project requires you to set environment variables in order to run on the AWS account: $ export DBT_ROLE_ARN=\u0026#34;arn:aws:iam::$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text):role/GlueInteractiveSessionRole\u0026#34; $ export DBT_S3_LOCATION=\u0026#34;s3://aws-dbt-glue-datalake-$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text)-us-east-1\u0026#34; Make sure the profile is set up correctly from the command line: $ dbt debug --profiles-dir profiles ... 05:34:22 Connection test: [OK connection ok] 05:34:22 All checks passed! If you see any failures, check if you provided the correct IAM role ARN and S3 location in Step 2. 4. Run the models with the following code:\n$ dbt run -m silver --profiles-dir profiles $ dbt run -m gold --profiles-dir profiles/iceberg/ Now the tables are successfully created in the SageMaker Lakehouse Catalog, and the data is materialized in the Amazon S3 location.\nYou can verify those tables by opening the AWS Glue console, choosing Databases in the navigation pane, and opening dbt_glue_demo_nyc_metrics. Query materialized tables through Athena Let’s query the target table using Athena to verify the materialized tables. Complete the following steps:\nOn the Athena console, switch the workgroup to athena-dbt-glue-aws-blog.\nIf the workgroup athena-dbt-glue-aws-blog settings dialog box appears, choose Acknowledge.\nUse the following query to explore the metrics created by the dbt project:\nSELECT cm.avg_cost_per_minute , cm.avg_cost_per_distance , pm.avg_cost_per_passenger , cm.year , cm.month , cm.type FROM \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34;.\u0026#34;gold_passengers_metrics\u0026#34; pm LEFT JOIN \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34;.\u0026#34;gold_cost_metrics\u0026#34; cm ON cm.type = pm.type AND cm.year = pm.year AND cm.month = pm.month WHERE cm.type = \u0026#39;yellow\u0026#39; AND cm.year = \u0026#39;2016\u0026#39; AND cm.month = \u0026#39;6\u0026#39; The following screenshot shows the results of this query. The following screenshot shows the results of this query. Complete the following steps to review your documentation:\nGenerate the following documentation for the project: $ dbt docs generate --profiles-dir profiles/iceberg 11:41:51 Running with dbt=1.7.1 11:41:51 Registered adapter: glue=1.7.1 11:41:51 Unable to do partial parsing because profile has changed 11:41:52 Found 3 models, 1 source, 0 exposures, 0 metrics, 478 macros, 0 groups, 0 semantic models 11:41:52 11:41:53 Concurrency: 1 threads (target=\u0026#39;dev\u0026#39;) 11:41:53 11:41:53 Building catalog 11:43:32 Catalog written to /Users/username/Documents/workspace/dbt_glue_demo/target/catalog.json Run the following command to open the documentation on your browser: $ dbt docs serve --profiles-dir profiles/iceberg In the navigation pane, choose gold_cost_metrics under dbt_glue_demo/models/gold. You can see the detailed view of the model gold_cost_metrics, as shown in the following screenshot. To see the lineage graph, choose the circle icon at the bottom right.\nClean up To clean up your environment, complete the following steps:\nDelete the database created by dbt: $ aws glue delete-database —name dbt_glue_demo_nyc_metrics\nDelete all generated data:\n$ aws s3 rm s3://aws-dbt-glue-datalake-$(aws sts get-caller-identity —query \u0026#34;Account\u0026#34; —output text)-us-east-1/ —recursive $ aws s3 rm s3://aws-athena-dbt-glue-query-results-$(aws sts get-caller-identity —query \u0026#34;Account\u0026#34; —output text)-us-east-1/ —recursive Delete the CloudFormation stack: $ aws cloudformation delete-stack —stack-name dbt-demo Conclusion This post demonstrated how the dbt-glue adapter helps your workload, and how you can build a modern data stack using dbt and AWS Glue using the dbt-glue adapter. You learned the end-to-end operations and data flow for data engineers to build and manage a data stack using dbt and the dbt-glue adapter.\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/5.4-data-catalog/5.4.1--data-lake/","title":"Data Lake Storage (Amazon S3)","tags":[],"description":"","content":"To store data in a durable and scalable way, the S3 bucket is organized into multiple layers.\nThe system connects to the S3 bucket using the credentials AWS_ACCESS_KEY_ID and AWS_SECRET_KEY, which are defined in the constants.py file, ensure exist bucket and then writes data into the bucket: def connect_to_s3(): try: s3= s3fs.S3FileSystem(anon=False, key=AWS_ACCESS_KEY_ID, secret=AWS_SECRET_KEY) return s3 except Exception as e: print(e) def create_bucket_if_not_exist(s3:s3fs.S3FileSystem,bucket:str): try: if not s3.exists(bucket): s3.mkdir(bucket) print(\u0026#34;Bucket created\u0026#34;) else: print(\u0026#34;Bucket already exists\u0026#34;) except Exception as e: print(\u0026#34;Error here\u0026#34;) print(e) def upload_to_s3(s3:s3fs.S3FileSystem,bucket:str,file_path: str,s3_file_name: str): try: s3.put(file_path,bucket+\u0026#39;/raw/\u0026#39;+ s3_file_name) print(\u0026#34;File upload to s3\u0026#34;) except FileNotFoundError: print(\u0026#34;The file was not found\u0026#34;) An Airflow task is created to upload data to S3: upload_s3 = PythonOperator( task_id = \u0026#39;s3_upload\u0026#39;, python_callable = upload_s3_pipeline, dag=dag ) extract \u0026gt;\u0026gt; upload_s3 The python_callable function upload_s3_pipeline performs the upload process: def upload_s3_pipeline(ti): file_path = ti.xcom_pull(task_ids= \u0026#39;reddit_extraction\u0026#39;,key = \u0026#39;return_value\u0026#39;) s3 =connect_to_s3() create_bucket_if_not_exist(s3, AWS_BUCKET_NAME) upload_to_s3(s3, AWS_BUCKET_NAME,file_path, file_path.split(\u0026#39;/\u0026#39;)[-1]) "},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/5.3-ingestion/5.3.1-docker-configuration/","title":"Docker Configuration","tags":[],"description":"","content":" Building a Custom Airflow Image A custom Airflow Docker image is built using the following Dockerfile:: FROM apache/airflow:2.8.4-python3.10 USER root RUN apt-get update \u0026amp;\u0026amp; apt-get install -y gcc python3-dev USER airflow COPY requirements.txt /opt/airflow/requirements.txt RUN pip install --no-cache-dir -r /opt/airflow/requirements.txt A dedicated Docker image is created based on: apache/airflow:2.8.4-python3.10\nThis image ensures that all Airflow containers run in a consistent runtime environment with the required Python dependencies.\n2 . Shared Configuration Definition A shared configuration block (x-airflow-common) is defined to:\nUse the same Airflow image Share environment variables Mount configuration, DAGs, logs, and pipeline directories x-airflow-common: \u0026amp;airflow-common build: context: . dockerfile: Dockerfile image: custom-airflow:2.8.4-python3.10 env_file: - airflow.env volumes: - ./config:/opt/airflow/config - ./dags:/opt/airflow/dags - ./data:/opt/airflow/data - ./etls:/opt/airflow/etls - ./logs:/opt/airflow/logs - ./pipelines:/opt/airflow/pipelines - ./plugins:/opt/airflow/plugins - ./tests:/opt/airflow/tests - ./utils:/opt/airflow/utils - ./requirements.txt:/opt/airflow/requirements.txt depends_on: - postgres - redis Deploying Core Services PostgreSQL (Metadata Database): Stores DAG definitions, task states, and execution history. postgres: image: postgres:12 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: postgres POSTGRES_DB: airflow_reddit ports: - \u0026#34;5432:5432\u0026#34; Redis (Message Broker): Enables multiple Celery workers to process tasks in parallel and retry failed tasks. redis: image: redis:latest ports: - \u0026#34;6379:6379\u0026#34; Airflow Initialization (airflow-init): The airflow-init service initializes the metadata database and creates the admin account. airflow-init: \u0026lt;\u0026lt;: *airflow-common command: \u0026gt; bash -c \u0026#34; pip install -r /opt/airflow/requirements.txt \u0026amp;\u0026amp; airflow db migrate \u0026amp;\u0026amp; airflow db upgrade \u0026amp;\u0026amp; airflow db check \u0026amp;\u0026amp; airflow users create --username admin --firstname admin --lastname admin --role Admin --email nguyenthihong112000@gmail.com --password admin\u0026#34; restart: \u0026#34;no\u0026#34; After this step completes, the Airflow system is ready to run. Deploying Airflow Components Webserver: Provides the web interface for managing DAGs, monitoring tasks, and viewing logs. airflow-webserver: \u0026lt;\u0026lt;: *airflow-common command: airflow webserver ports: - \u0026#34;8080:8080\u0026#34; Scheduler: Schedules tasks and sends them to the execution queue. airflow-scheduler: \u0026lt;\u0026lt;: *airflow-common command: airflow scheduler Celery Worker: Executes tasks such as calling the Reddit API and uploading data to Amazon S3. airflow-worker: \u0026lt;\u0026lt;: *airflow-common container_name: airflow-worker command: airflow celery worker ports: - \u0026#34;8793:8793\u0026#34; Triggerer: Handles asynchronous tasks and sensors. airflow-triggerer: \u0026lt;\u0026lt;: *airflow-common command: airflow triggerer "},{"uri":"https://roses19.github.io/fcj-workshop-roses/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Thi Anh Hong\nPhone Number: 0327761048\nEmail: 2251052039hong@ou.edu.vn\nUniversity: Ho Chi Minh City Open University\nMajor: Information Technology\nClass: DH22IT02\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Data Intern\nInternship Duration: From 15/12/2025 to 17/1/2026\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Data Platform A Data Platform is a system that enables the collection, storage, processing, and analysis of large-scale data in an automated and scalable manner. In this architecture, data is organized into layers (raw, transformed, analytics) to ensure that the original data is preserved while optimizing it for analysis and reporting.\nWorkshop overview In this workshop, Reddit data is analyzed on AWS based on a Data Lake architecture combined with a Data Warehouse.\nThe system includes:\nApache Airflow running in Docker to collect data from the Reddit API and orchestrate the pipeline.\nAmazon S3 to store data in two layers: Raw and Transformed.\nAWS Glue \u0026amp; Data Catalog to automatically discover schemas and process data.\nAmazon Athena to query data directly from S3.\nAmazon Redshift to store analytical data.\nAmazon QuickSight to build dashboards and visualize data.\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Master Amazon S3 storage services, security mechanisms, and distribution optimization through CloudFront. Establish core infrastructure and security permissions for the Reddit Data Pipeline project (Airflow, S3, IAM). Build and successfully operate an automated ETL flow to ingest data from Reddit into the Data Lake. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore Amazon S3 Essential. - Practice Static Website Hosting:\n+ Configure Static Website Hosting, set up Index \u0026amp; Error documents + Access Management: Block Public Access, Bucket Policies, and ACLs. 15/12/2025 15/12/2025 https://000057.awsstudygroup.com/ 3 - Data Management \u0026amp; Optimization: + Enable Versioning to protect data from accidental deletion. + Set up CRR for disaster recovery. + Accelerate website with CDN and configure OAC. 16/12/2025 16/12/2025 https://000057.awsstudygroup.com/vi/7-cloudfront/ 4 - Project Infrastructure \u0026amp; Security Setup: + Initialize Airflow environment. + Create S3 Buckets . + Configure IAM Roles/Policies for Airflow, S3, and Athena. + Register Reddit App for Client ID/Secret. 17/12/2025 18/12/2025 5 - Explore AWS Glue and Athena services. - Practice: + Create appropriate IAM Roles and Policies. + Create S3 buckets, ETL jobs, and Glue Crawlers. 18/12/2025 18/12/2025 https://000035.awsstudygroup.com/vi/ 6 - Data Discovery (Glue \u0026amp; Athena): + Run Glue Crawler to scan raw data schemas on S3. + Use Athena to execute SQL queries and verify uploaded data. - ETL Pipeline Development: + Write Airflow DAG to crawl data from Reddit API. + Implement data ingestion flow to S3 Raw zone. 19/12/2025 19/12/2025 Week 1 Achievements: Storage Management \u0026amp; Content Delivery:\nMastered Amazon S3 architecture and successfully deployed Static Website Hosting with complete Index and Error document configurations. Implemented strict security layers via Block Public Access and Bucket Policies; protected data using Versioning and Cross-Region Replication (CRR). Optimized website performance by integrating CloudFront CDN and utilizing Origin Access Control (OAC) to secure data sources. Reddit Project Infrastructure Setup:\nSuccessfully initialized the Apache Airflow orchestration environment and registered Reddit API credentials (Client ID/Secret). Built a tiered storage structure on S3 (Raw, Transformed, Athena) following the standard Data Lake architecture model. Configured specialized IAM Roles and Policies for Airflow and Athena, adhering to the principle of least privilege. Data Discovery \u0026amp; ETL Workflow:\nOperated AWS Glue Crawler to automatically scan schemas and update Metadata for raw datasets stored on S3. Successfully executed ad-hoc SQL queries on Amazon Athena to verify the integrity and consistency of ingested data. Developed a complete initial Airflow DAG, automating the process of crawling Reddit data and ingesting it into the Raw zone of the Data Lake. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Master data cleansing, transformation, and optimization using AWS Glue and PySpark. Understand and apply Monitoring, Cost Management, and Security for a cloud data platform. Learn core AWS Data \u0026amp; Analytics services including S3, Glue, Athena, and QuickSight. Build and manage a Data Lake and ETL pipeline on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - ETL Transformation:\n+ Build Glue Jobs (PySpark) to clean data.\n+ Convert data formats to optimize performance. 22/12/2025 22/12/2025 3 - Monitoring \u0026amp; Cost Management\n+ CloudWatch logs \u0026amp; metrics\n+ AWS Budgets\n+ Cost Explorer 23/12/2025 23/12/2025 https://000008.awsstudygroup.com/ 4 - IAM \u0026amp; Security for Data Platform\n+ IAM User vs Role vs Policy\n+ IAM for Glue, Athena, Redshift\n+ S3 Bucket Policy vs IAM Policy\n+ KMS \u0026amp; data encryption 24/12/2025 24/12/2025 https://000033.awsstudygroup.com/ 5 - Networking for AWS Data Services\n+ VPC, Subnet, Internet Gateway\n+ VPC Endpoints for S3 \u0026amp; Glue\n+ Public vs Private access\n+ How AWS data services communicate 25/12/2025 26/12/2025 https://000003.awsstudygroup.com/vi/ 6 - Build an AWS Data Lake: Learn AWS Glue, Athena, QuickSight.\n- Hands-on:\n+ Create appropriate IAM roles and policies.\n+ Create S3 buckets, ETL jobs, and crawlers.\n+ Query data using Athena SQL. 26/12/2025 26/12/2025 https://000035.awsstudygroup.com/vi Week 2 Achievements: Built AWS Glue ETL Jobs (PySpark) to:\nClean Reddit data Normalize schemas for analytics Move data from Raw S3 → Processed S3 Implemented Monitoring \u0026amp; Cost Management:\nEnabled CloudWatch Logs for Glue Jobs Set up AWS Budgets to track spending Used Cost Explorer to analyze usage Configured Security for the Data Platform:\nIAM Roles for Glue, Athena, and S3 S3 Bucket Policies following the least-privilege principle Enabled KMS encryption for data Designed and understood Networking for AWS Data Services:\nVPC, Subnets, and Internet Gateway VPC Endpoints for S3 and Glue Differences between Public and Private access Built an AWS Data Lake:\nCreated S3 structure: raw/, processed/ Ran Glue Crawler to generate the Data Catalog "},{"uri":"https://roses19.github.io/fcj-workshop-roses/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Deploy a data warehouse and perform high-performance analytics with Redshift Serverless. Design professional interactive BI dashboards using Amazon QuickSight. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Set up \u0026amp; explore Amazon Redshift + Overview of Data Warehousing \u0026amp; Redshift Serverless + Create Namespace \u0026amp; Workgroup + Configure IAM Role for S3 access 29/12/2025 30/12/2025 https://docs.aws.amazon.com/redshift/ 3 Load \u0026amp; analyze data in Redshift + Create schemas and tables + Use the COPY command to load data from S3 + Perform analytical SQL using Query Editor v2 30/12/2025 30/12/2025 https://docs.aws.amazon.com/redshift/ 4 Amazon QuickSight \u0026amp; BI Layer Learn core components: SPICE, Dataset, Analysis, and Dashboard Hands-on: + Prepare datasets and set up the environment in the Singapore region + Create visualizations + Optimize layouts and enable interactivity + Publish dashboards and clean up resources 31/12/2025 31/12/2025 https://000073.awsstudygroup.com/ Week 3 Achievements: AWS Redshift Serverless:\nInitialized the Namespace/Workgroup environment and successfully loaded data from S3 into tables via the COPY command. Leveraged Query Editor v2 for in-depth SQL analytics and performance tuning on the data warehouse. Amazon QuickSight:\nOptimized data processing performance using the SPICE in-memory calculation engine. Implemented various visualization types, including Line charts, KPI \u0026amp; Insights, Pie charts, and Pivot Tables. Configured flexible dashboard interactivity through Filters and Navigation Actions. Successfully published the final Dashboard and executed resource cleanup to optimize operational costs. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Expand knowledge of AWS Data Platform architecture. Learn how large-scale data systems are automated, monitored, and operated. Perform labs and hands-on exercises to explore advanced AWS services. Evaluate the architecture and cost of the current Reddit Data Platform. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2–3 Data automation architecture + Event-driven vs scheduled pipelines + AWS Lambda \u0026amp; EventBridge + Comparison with Airflow 05/01/2026 06/01/2026 https://docs.aws.amazon.com/eventbridge/ 4 Hands-on: Monitoring \u0026amp; Observability + CloudWatch logs \u0026amp; metrics + Dashboards \u0026amp; alerts + Pipeline SLA 07/01/2026 07/01/2026 https://000008.awsstudygroup.com/vi/ 5 System architecture review + Review data flow: Reddit → S3 → Glue → Athena → Redshift → QuickSight + Identify bottlenecks and risks 08/01/2026 08/01/2026 6 Cost \u0026amp; performance analysis + Glue, S3, Athena, Redshift, QuickSight + Propose optimization strategies 09/01/2026 09/01/2026 Week 4 Achievements: Gained a clear understanding of pipeline automation models (event-driven and scheduled) used in modern data platforms. Practiced monitoring, logging, and alerting for the data platform using CloudWatch. Evaluated the overall architecture of the Reddit Data Platform and identified potential bottlenecks. Analyzed the cost and performance of AWS services in use (Glue, S3, Athena, Redshift, QuickSight). Identified optimization and future extension directions for the next phase. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Complete the final report and project summary for the Reddit Data Platform. Evaluate the architecture, performance, and cost of the current system. Study advanced Data Platform models to define future expansion directions. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Write the architecture and data flow report + Reddit → S3 → Glue → Athena → Redshift → QuickSight + Draw architecture diagrams 12/01/2026 12/01/2026 3 System review \u0026amp; evaluation + Performance evaluation + Cost analysis + Results summary 13/01/2026 13/01/2026 4 Study advanced Data Platform architectures + Batch vs Streaming + Kinesis \u0026amp; Firehose + Data Lakehouse 14/01/2026 14/01/2026 https://000072.awsstudygroup.com/vi/10-kinesis-data-analytics/ 5 Explore system extensions 15/01/2026 15/01/2026 6 Future direction \u0026amp; roadmap + Design future architecture + Propose improvements + Project roadmap 16/01/2026 16/01/2026 Week 5 Achievements: Completed a full technical report describing the Reddit Data Platform. Summarized and evaluated the cost, performance, and architecture of the system. Identified limitations and improvement areas of the current pipeline. Gained knowledge of modern Data Platform models such as streaming, lakehouse, and multi-source ingestion. Built a future expansion roadmap for the system. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This section documents the 5-week journey of building the Reddit Data Pipeline while exploring and practicing services on AWS.\nHere, I share milestones starting from secure cloud infrastructure setup and automated pipeline construction to the final data visualization stage.\nThe weekly tasks are summarized as follows:\nWeek 1 – Reddit Data Ingestion\nWeek 2 – Data Processing \u0026amp; Storage\nWeek 3 – Data Analytics \u0026amp; Visualization\nWeek 4 – Automation, Monitoring \u0026amp; System Review\nWeek 5 – Reporting \u0026amp; Architecture Extension\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AWS FIRST CLOUD JOURNEY COMMUNITY DAY 2025” Event Objectives Speakers Key Highlights "},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/5.4-data-catalog/5.4.2-data-cataloging-pipeline/","title":"Data Cataloging Pipeline","tags":[],"description":"","content":"1. Create a Glue Database The Glue Database is used to group Reddit tables and centrally manage metadata 2. Create a Glue Job Data stored in S3 (Raw Zone) needs to be standardized, cleaned, and optimized before being queried. The job is configured with Source from raw layer (s3://amzn-s3-reddit-airflow-project/raw/ \u0026lt;raw_files\u0026gt;) and target from transformed layer (s3://amzn-s3-reddit-airflow-project/transformed/):\nAdditional scripts are written to standardize the data:\nThe data is read from S3 as a DynamicFrame, then converted to a Spark DataFrame to perform transformations. from awsglue import DynamicFrame from pyspark.sql.functions import concat_ws Three attributes — edited, spoiler, and stickied — are combined into a single column ESS_updated to simplify the schema and create a composite analytical feature.\nAfter processing, the data is converted back into a DynamicFrame.\nFinally, the standardized data is written to the Transformed Zone (S3), ready for querying and analysis using Athena and Redshift.\n#convert DynamicFrame to DataFrame df = AmazonS3_node1766723555757.toDF() #concatenate three columns into a single columns df_combined = df.withColumn(\u0026#39;ESS_updated\u0026#39;, concat_ws(\u0026#39;-\u0026#39;, df[\u0026#39;edited\u0026#39;], df[\u0026#39;spoiler\u0026#39;], df[\u0026#39;stickied\u0026#39;])) df_combined = df_combined.drop(\u0026#39;edited\u0026#39;,\u0026#39;spoiler\u0026#39;,\u0026#39;stickied\u0026#39;) #convert back to DynamicFrame S3bucket_node_combined = DynamicFrame.fromDF(df_combined,glueContext,\u0026#39;S3bucket_node_combined\u0026#39;) # Script generated for node Amazon S3 EvaluateDataQuality().process_rows(frame=S3bucket_node_combined, ruleset=DEFAULT_DATA_QUALITY_RULESET, publishing_options={\u0026#34;dataQualityEvaluationContext\u0026#34;: \u0026#34;EvaluateDataQuality_node1766723551899\u0026#34;, \u0026#34;enableDataQualityResultsPublishing\u0026#34;: True}, additional_options={\u0026#34;dataQualityResultsPublishing.strategy\u0026#34;: \u0026#34;BEST_EFFORT\u0026#34;, \u0026#34;observations.scope\u0026#34;: \u0026#34;ALL\u0026#34;}) AmazonS3_node1766723556784 = glueContext.write_dynamic_frame.from_options(frame=S3bucket_node_combined, connection_type=\u0026#34;s3\u0026#34;, format=\u0026#34;csv\u0026#34;, connection_options={\u0026#34;path\u0026#34;: \u0026#34;s3://amzn-s3-reddit-airflow-project/transformed/\u0026#34;, \u0026#34;partitionKeys\u0026#34;: []}, transformation_ctx=\u0026#34;AmazonS3_node1766723556784\u0026#34;) job.commit()\tChoose an appropriate IAM Role that includes the following policies AmazonS3ReadOnlyAccess, AWSGlueServiceRole, and Custom inline policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::amzn-s3-reddit-airflow-project\u0026#34;, \u0026#34;arn:aws:s3:::amzn-s3-reddit-airflow-project/*\u0026#34; ] } ] } After the job runs successfully, the cleaned and standardized data is stored in the Transformed Zone: 2. Create a Glue Crawler The crawler is configured as follows:\nName: reddit_crawler Data source: Amazon S3 Path: s3://amzn-s3-reddit-airflow-project/transformed/ (the standardized data layer) IAM role:AWSGlueServiceRole-Reddit-glue-role Database name: reddit_db Result: The crawler scans all standardized filesfilesfilesfiles, automatically detects the schema, and stores table metadata in the Glue Data Catalog\nWhen the crawler runs, Glue reads the transformed data and generates tables, which are then registered in the Glue Catalog. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/5.3-ingestion/5.3.2-ingestion-pipeline/","title":"Ingestion Pipeline","tags":[],"description":"","content":"Reddit API Connection The system uses the Reddit API through a Developer script-type application to collect posts and comments from subreddits such as dataengineering.\nApache Airflow uses the PRAW library to handle: OAuth authentication, data retrievalretrieval, compliance with Reddit API request limits\ndef connect_reddit(client_id, client_secret, user_agent) -\u0026gt; Reddit: try: reddit = praw.Reddit(client_id = client_id, client_secret = client_secret, user_agent = user_agent) print(\u0026#34;Connect to Reddit\u0026#34;) return reddit except Exception as e: print(e) sys.exit(1) Reddit ETL DAG in Airflow Apache Airflow is used to orchestrate the entire Reddit data collection and storage process.\nA DAG and a scheduled task are defined as follows:\ndag = DAG( dag_id=\u0026#39;etl_reddit_pipeline\u0026#39;, default_args=default_args, schedule_interval=\u0026#39;@daily\u0026#39;, catchup=False, tags=[\u0026#39;reddit\u0026#39;, \u0026#39;etl\u0026#39;, \u0026#39;pipeline\u0026#39;] ) Docker is used to verify that the DAG is running successfully (the airflow-init service must be executed first to initialize the database and create the Airflow user). DAG successfully executed Reddit Data Processing The extracted data includes the following fields: id, title, score, num_comments, author, created_utc, url, over_18, edited, spoiler, stickied\nThe extract_posts function queries the Reddit API via PRAW and retrieves posts from a given subreddit based on a time filter.\ndef extract_posts (reddit_instance: Reddit, subreddit:str, time_filter:str, limit = None): subreddit = reddit_instance.subreddit(subreddit) posts = subreddit.top(time_filter = time_filter, limit=limit) post_lists = [] for post in posts: post_dict = vars(post) post = {key: post_dict[key] for key in POST_FIELDS} post_lists.append(post) return post_lists Each post is converted into a dictionary and only the required fields are retained to reduce data size and optimize downstream processing. The transform_data function performs data normalization before storing it in the Data Lake. def transform_data (post_df: pd.DataFrame): post_df[\u0026#39;created_utc\u0026#39;] = pd.to_datetime(post_df.get(\u0026#39;created_utc\u0026#39;, None),unit=\u0026#39;s\u0026#39;,errors=\u0026#39;coerce\u0026#39;) post_df[\u0026#39;over_18\u0026#39;] = post_df.get(\u0026#39;over_18\u0026#39;, False) post_df[\u0026#39;over_18\u0026#39;] = post_df[\u0026#39;over_18\u0026#39;].fillna(False).astype(bool) post_df[\u0026#39;author\u0026#39;] = post_df[\u0026#39;author\u0026#39;].astype(str) edited_mode = post_df[\u0026#39;edited\u0026#39;].mode() post_df[\u0026#39;edited\u0026#39;] = np.where(post_df[\u0026#39;edited\u0026#39;].isin([True, False]), post_df[\u0026#39;edited\u0026#39;],edited_mode).astype(bool) post_df[\u0026#39;num_comments\u0026#39;] = post_df[\u0026#39;num_comments\u0026#39;].astype(int) post_df[\u0026#39;score\u0026#39;] = post_df[\u0026#39;score\u0026#39;].astype(int) post_df[\u0026#39;title\u0026#39;] = post_df[\u0026#39;title\u0026#39;].astype(str) return post_df Tóm tắt This section completes the data ingestion phase. Reddit data has been successfully collected and normalized before being loaded into the Data Lake. The raw API data is transformed into structured tabular format, making it ready for storage on Amazon S3 and for querying through Athena and Redshift in the next stages of the pipeline. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;events:*\u0026#34;, \u0026#34;schemas:*\u0026#34;, \u0026#34;scheduler:*\u0026#34;, \u0026#34;pipes:*\u0026#34;, \u0026#34;redshift:*\u0026#34;, \u0026#34;redshift-serverless:*\u0026#34;, \u0026#34;redshift-data:*\u0026#34;, \u0026#34;sqlworkbench:*\u0026#34;, \u0026#34;athena:*\u0026#34;, \u0026#34;glue:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;s3-object-lambda:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;logs:*\u0026#34;, \u0026#34;observabilityadmin:*\u0026#34;, \u0026#34;ec2:*\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:ListAliases\u0026#34;, \u0026#34;secretsmanager:*\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:ListRolePolicies\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ChangePassword\u0026#34;, \u0026#34;iam:GetAccountPasswordPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;redshift:CreateClusterUser\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:redshift:*:*:dbuser:*/redshift_data_api_user\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iam:CreateServiceLinkedRole\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:iam::*:role/aws-service-role/redshift.amazonaws.com/AWSServiceRoleForRedshift\u0026#34;, \u0026#34;arn:aws:iam::*:role/aws-service-role/redshift-data.amazonaws.com/AWSServiceRoleForRedshift\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;iam:AWSServiceName\u0026#34;: [ \u0026#34;redshift.amazonaws.com\u0026#34;, \u0026#34;redshift-data.amazonaws.com\u0026#34; ] } } } ] } Creating a Reddit API Application for Apache Airflow A Script-type application is created on the Reddit Developer Portal in order to obtain a Client ID and Client Secret. These credentials are used to authenticate Apache Airflow when calling the Reddit API. (The Reddit account used to create the application must be verified and older than 10 days for security reasons.)\nInitializing the S3 Data Lake Structure In this lab, the N. Virginia region (us-east-1) is used.\nIn this project, the Data Lake is built on Amazon S3, serving as the central storage layer for all Reddit data. It supports batch processing, historical analysis, and BI queries.\nThe Data Lake is designed with multiple layers to clearly separate each stage of the data lifecycle:\nDefining Variables in config.conf Purpose: to provide a centralized configuration and ensure secure credential management. [database] database_host = localhost database_name = airflow_reddit database_port = 5432 database_username = postgres database_password = postgres [file_paths] input_path = /opt/airflow/data/input output_path = /opt/airflow/data/output [api-keys] reddit_secret_key = [your secret key] reddit_client_key = [your client key] [aws] aws_access_key_id = [your-aws-key] aws_secret_access_key_id = [your-secret-key] aws_region = us-east-1 aws_bucket_name = amzn-s3-reddit-airflow-project [etl_settings] batch_size = 100 error_handling = abort log_level = info Mapping Variables in constants.py These values are then loaded into the application via constants.py: import configparser import os parse = configparser.ConfigParser() parse.read(os.path.join(os.path.dirname(__file__), \u0026#39;../config/config.conf\u0026#39;)) SECRET = parse.get(\u0026#39;api-keys\u0026#39;, \u0026#39;reddit_secret_key\u0026#39;) CLIENT_ID = parse.get(\u0026#39;api-keys\u0026#39;, \u0026#39;reddit_client_key\u0026#39;) DATABASE_HOST = parse.get(\u0026#39;database\u0026#39;, \u0026#39;database_host\u0026#39;) DATABASE_NAME = parse.get(\u0026#39;database\u0026#39;, \u0026#39;database_name\u0026#39;) DATABASE_PORT = parse.get(\u0026#39;database\u0026#39;, \u0026#39;database_port\u0026#39;) DATABASE_USER = parse.get(\u0026#39;database\u0026#39;, \u0026#39;database_username\u0026#39;) DATABASE_PASSWORD = parse.get(\u0026#39;database\u0026#39;, \u0026#39;database_password\u0026#39;) #aws AWS_ACCESS_KEY_ID = parse.get(\u0026#39;aws\u0026#39;,\u0026#39;aws_access_key_id\u0026#39;) AWS_SECRET_KEY = parse.get(\u0026#39;aws\u0026#39;, \u0026#39;aws_secret_access_key_id\u0026#39;) AWS_REGION = parse.get(\u0026#39;aws\u0026#39;, \u0026#39;aws_region\u0026#39;) AWS_BUCKET_NAME = parse.get(\u0026#39;aws\u0026#39;,\u0026#39;aws_bucket_name\u0026#39;) INPUT_PATH = parse.get(\u0026#39;file_paths\u0026#39;, \u0026#39;input_path\u0026#39;) OUTPUT_PATH = parse.get(\u0026#39;file_paths\u0026#39;, \u0026#39;output_path\u0026#39;) POST_FIELDS = ( \u0026#39;id\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;score\u0026#39;, \u0026#39;num_comments\u0026#39;, \u0026#39;author\u0026#39;, \u0026#39;created_utc\u0026#39;, \u0026#39;url\u0026#39;, \u0026#39;over_18\u0026#39;, \u0026#39;edited\u0026#39;, \u0026#39;spoiler\u0026#39;, \u0026#39;stickied\u0026#39; ) Setting up Amazon Redshift Serverless Amazon Redshift Serverless is used as the central data warehouse for Reddit analytics.\nUsing Redshift Serverless eliminates the need to manage physical clusters while allowing the system to automatically scale compute resources based on query demand.\nThe setup process includes:\nCreating a Workgroup to provide compute resources for query execution\nNamespace: reddit-namespace\nWorkgroup: reddit-workgroup\nAfter the setup is completed, Redshift Serverless is ready to serve as the central Data Warehouse, directly integrated with the S3 Data Lake and the Glue Data Catalog to support subsequent processing and analytics steps.\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/2-proposal/","title":"Proposal","tags":[],"description":"","content":"AWS Reddit ETL Platform Automating the ETL Pipeline for Reddit Data on AWS 1. Executive Summary This project is designed to build an automated social media data collection and processing system that optimizes the balance between available on-premise resources and cloud infrastructure. The system adopts a Hybrid Architecture, where Apache Airflow runs on a physical local server to handle orchestration, while AWS Serverless services are used to process and store large-scale data.\nThe solution fully automates the Reddit data lifecycle—from data ingestion, automatic schema discovery using AWS Glue Crawlers, to Spark-based ETL transformations that convert data into Parquet format, significantly reducing query costs. The system leverages Amazon Redshift Serverless with a minimal configuration (4 RPUs) for high-performance analytics and visualization through Amazon QuickSight Standard, minimizing monthly operating costs while maintaining analytical flexibility.\n2. Problem Statement What’s the Problem?\nExtracting and analyzing data from social media platforms faces several key challenges:\nFragmented infrastructure – Data scraping and analytics are often disconnected, causing stale or lost data due to manual handling.\nInefficient compute utilization – Running a 24/7 data warehouse for infrequent queries results in unnecessary costs, especially for small and medium-sized projects.\nAPI rate limits and costs – Reddit enforces strict API limits (100 QPM), requiring intelligent orchestration to avoid account suspension.\nThe Solution\nA modernized data pipeline is proposed:\nLocal orchestration – Airflow runs in Docker on an on-premise server to eliminate cloud compute costs for orchestration.\nServerless ingestion \u0026amp; ETL – Amazon S3 and AWS Glue handle large-scale unstructured data processing without server management.\nOptimized analytics – Redshift Serverless with 4 RPUs executes SQL analytics only when needed, delivering high performance at very low cost.\nBenefits and Return on Investment\nThis hybrid solution reduces fixed infrastructure costs by approximately 30–40% compared to a fully cloud-based deployment. Operational expenses occur only when AWS services are used, averaging USD 25–30 per month. The time from breaking Reddit news to visualization in QuickSight is reduced from days to hours, enabling near-real-time insights.\n3. Solution Architecture The system applies an event-driven, serverless architecture combined with a container-based local orchestration layer. AWS Services Used\nAmazon S3 – Serves as the Data Lake for both raw and transformed data AWS Glue Crawler – Automatically detects schema and updates metadata AWS Glue Data Catalog – Centralized metadata repository AWS Glue ETL – Serverless Spark processing for data cleansing and Parquet conversion Amazon Athena – Serverless SQL querying directly on S3 Amazon Redshift Serverless – Auto-scaling data warehouse for high-performance analytics Amazon QuickSight Standard – Serverless BI platform for dashboarding Component Design\nIngestion – Airflow (Local Docker) pulls Reddit data via API and stores raw JSON in S3\nProcessing – Glue Crawler updates schema, and Glue Spark ETL converts JSON to partitioned Parquet\nAnalytics – Data is queried in Redshift Serverless and visualized in QuickSight dashboards\n4. Technical Implementation Implementation Phases\nLocal Environment Setup – Configure Docker Compose for Airflow and establish secure IAM access to AWS Reddit Extractor Development – Implement Python scripts using PRAW with exponential backoff to respect API limits Glue Pipeline – Configure Crawlers and Spark ETL jobs to clean and partition Parquet data by date Data Warehouse Setup – Create Redshift Serverless namespace and workgroup with 4 RPUs Dashboard Design – Connect QuickSight to Redshift and build analytics dashboards Technical Requirements\nReddit API – Script-type developer account with 100 QPM rate limit Compute (Orchestration) – Local server with at least 8 GB RAM for Docker \u0026amp; Celery workers Storage – Parquet format in S3 reduces storage and query costs by up to 90% compared to JSON Redshift Serverless – Minimal 4 RPU configuration for cost-efficient analytics QuickSight Standard – Low-cost BI for basic reporting 5. Timeline \u0026amp; Milestones Project Timeline\nPre-internship (Month 0) – AWS fundamentals and ETL architecture training Internship Phase: Weeks 1–2: AWS learning and hardware upgrade Week 3: Architecture design and tuning Weeks 4–5: Deployment, testing, and production rollout Post-deployment – 6-month research phase for system enhancement 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nInfrastructure Costs\nAWS Services: Amazon S3 (Data Lake): USD 0.17/month (6 GB, 5,000 requests, 1 GB scanned) Data Transfer: USD 0.54/month (6 GB inbound, 6 GB outbound) AWS Glue ETL Jobs: USD 3.09/month (2 DPUs, Spark and Python Shell jobs) AWS Glue Crawlers: USD 0.66/month (1 crawler) AWS Glue Data Catalog: USD 0.01/month Amazon Athena: USD 0.74/month (10 queries/day, 0.5 GB per query) Amazon Redshift Serverless: USD 7.32/month (4 RPUs, ~0.16 hours/day) Total: $15.38/month ≈ $ 184.56/year\nAmazon QuickSight (demo only): USD 9–12/month (1 Author, no SPICE, deleted after demo) 7. Risk Assessment Local server availability – Failure may interrupt ingestion\nMitigation: Enable Airflow catch-up to rerun missed tasks API rate limits – Exceeding 100 QPM may block access\nMitigation: Limit to 5–10 subreddits and use Celery queues Schema drift – Reddit data structure may change\nMitigation: Use Glue Crawlers for automatic schema discovery 8. Expected Outcomes Fully automated Reddit data ingestion AWS cost optimized to stay below USD 30/month Easily scalable by adding new subreddits via Airflow configuration Interactive dashboards providing real-time insights into social media trends. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Build and manage your modern data stack using dbt and AWS Glue through dbt-glue, the new \u0026ldquo;trusted\u0026rdquo; dbt adapter The blog explains how the dbt-glue adapter helps optimize data workflows and how to build and manage a modern data platform by combining dbt with AWS Glue. It provides a clear end-to-end view of how to implement a modern data stack using dbt and the dbt-glue adapter.\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/5.3-ingestion/","title":"Ingestion","tags":[],"description":"","content":"Using Airflow During the data ingestion phase, the system uses Apache Airflow as the orchestration platform to automate the process of collecting data from the Reddit API and loading it into the Data Lake.\nThe entire Airflow environment is deployed using Docker, and consists of the main components: Webserver, Scheduler, Workers, PostgreSQL, and Celery.\nContent Docker Configuration Ingestion pipeline "},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/5.4-data-catalog/","title":"Data Ingestion and Cataloging Pipeline","tags":[],"description":"","content":"Overview After the data is collected by Airflow, the system loads it into the Data Lake on Amazon S3 and uses AWS Glue to automatically generate metadata.\nAmazon S3 serves as the central storage for Reddit data.\nAWS Glue Crawler scans the data and creates tables in the Glue Data Catalog.\nAs a result, raw data files are transformed into queryable tables that can be accessed by Amazon Athena and Amazon Redshift.\nThis process automates data ingestion, schema management, and provides the foundation for downstream analytics. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Before starting my internship, I participated in the kick-off event. It was a memorable experience, where I learned from the insights shared by senior members, explored inspiring and practical projects, and gained strong motivation to continue discovering new knowledge at AWS.\nEvent 1 Tên sự kiện: AWS FIRST CLOUD JOURNEY COMMUNITY DAY 2025\nThời gian: 09:00 AM, August 30, 2025\nĐịa điểm: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nVai trò trong sự kiện: Participant\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/5.5-analytics/","title":"Analytics and Visualization","tags":[],"description":"","content":"1. Querying Data with Amazon Athena After the Glue Crawler creates tables in the Glue Data Catalog, data stored in the S3 Transformed Zone can be queried directly using Amazon Athena without moving the data.\nAthena uses metadata from the Glue Data Catalog to understand: Schema, data types, and file locations in S3\nAthena configuration\nDatabase: reddit_db\nTable: transformed\nQuery settings → query results are stored at: s3://amzn-s3-reddit-airflow-project/athena-script/\nExample query: SELECT author, COUNT(*) AS total_posts, AVG(score) AS avg_score, SUM(num_comments) AS total_comments FROM \u0026#34;AwsDataCatalog\u0026#34;.\u0026#34;reddit_db\u0026#34;.\u0026#34;transformed\u0026#34; GROUP BY author ORDER BY total_posts DESC; Result: In the early stage of analysis, Athena is used for rapid exploratory analysis, data inspection and validation of ETL quality. 2. Loading Data from S3 into Amazon Redshift Although Athena is suitable for ad-hoc queries, advanced analytics such as BI dashboards, large joins, and high-performance queries require loading the data into Amazon Redshift (Data Warehouse).\nWith the previously prepared configurations (namespace and workgroup), the data is queried using Query Editor v2: The connection is established to the created workgroup using Federated user authentication, which eliminates the need for database passwords: Data is then loaded from S3 into the Data Warehouse: The public schema is selected, a table name is assigned in the warehouse, and the default IAM role is used: The Data Warehouse after a successful load: 3. Future Direction for Visualization and Advanced Analytics Although the current volume of Reddit data is not yet large enough to fully exploit Business Intelligence (BI) systems, the platform architecture has been designed to be scalable and future-ready.\nAs data continues to be collected over time through daily ingestion via Airflow, Amazon Redshift will gradually evolve into a historical data warehouse with increasing temporal depth. At that stage, BI tools such as Amazon QuickSight can be integrated directly with Redshift to enable:\nVisualization of post growth trends by subreddit\nAnalysis of engagement behavior (score, comments, ESS_updated) over time\nMonitoring topic evolution and influence\nSupporting data-driven decision making using both historical and near real-time data\nTherefore, the current system not only supports data processing and querying, but also serves as a complete enterprise-grade analytics platform, ready to be extended with visualization and advanced analytics once the dataset reaches sufficient scale.\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Automating the ETL Pipeline for Reddit Data on AWS Overview This project builds an automated system for analyzing Reddit data on AWS with the following main components:\nData collection and storage: Data is automatically retrieved from the Reddit API using Apache Airflow and stored in Amazon S3 following the Data Lake model (Raw and Transformed).\nData processing and management: AWS Glue ETL Jobs are used to clean and transform the data, while AWS Glue Data Catalog and Crawler manage schemas and metadata.\nAnalytics and querying: Amazon Athena enables direct querying on the Data Lake, while Amazon Redshift serves as the Data Warehouse for analytical queries.\nVisualization: Amazon QuickSight is used to build dashboards and display insights from the data.\nThe system ensures that Reddit data is continuously updated and analyzed automatically, supporting the tracking of trends and user behavior on the AWS platform.\nContent Workshop overview Prerequiste Ingestion Data Catalog Analytics and Visualization Clean up "},{"uri":"https://roses19.github.io/fcj-workshop-roses/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd, I had the opportunity to participate in a data engineering project built on AWS, where data was collected from external APIs and processed through an ETL – Data Lake – Data Warehouse architecture.\nThrough this project, I applied the knowledge learned at university to a real-world system, including:\nCollecting data from the Reddit API using Python and Apache Airflow Storing raw data in Amazon S3 (Data Lake) Cleaning, transforming, and structuring data using AWS Glue Querying and exploring data with Amazon Athena Loading curated data into Amazon Redshift (Data Warehouse) for analytical workloads Visualizing insights using Amazon QuickSight This experience helped me understand the full lifecycle of data in a modern data platform, from ingestion and storage to processing, analytics, and visualization.\nI also gained hands-on experience in working with AWS cloud infrastructure, IAM permissions, logging, monitoring, and cost-aware resource management.\nIn terms of work attitude, I consistently completed assigned tasks on time, actively explored AWS documentation when encountering technical issues, and regularly communicated with my mentor and teammates to improve the quality and reliability of the system.\nTo objectively reflect on my internship performance, I evaluated myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Improvement in organizing tasks and managing time effectively Improved communication skills in daily and work interactions, as well as within teams Improvement in problem-solving thinking "},{"uri":"https://roses19.github.io/fcj-workshop-roses/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Resource Cleanup Because the Data Platform uses multiple compute-intensive services (such as AWS Glue and Amazon Redshift), costs can increase rapidly if these resources are not properly managed, even when there are no active users. Therefore, cleaning up resources after completing experiments is necessary to avoid unnecessary charges.\n1. Glue Crawler Delete the crawler to prevent Glue from continuing to scan data: Go to AWS Glue → Crawlers → reddit_crawler → Delete 2. Glue Job Delete the ETL job to stop Glue from consuming compute resources: Go to Glue → Jobs → reddit_glue_jobjob.py → Action → Delete job(s) 3. Glue Logs Delete Glue execution logs to avoid storage charges in CloudWatch: Go to CloudWatch → Log groups → /aws-glue/* → Delete group logs 4. Delete S3 Buckets Open the Amazon S3 console\nSelect the buckets created for the project (raw, transformed, athena, and glue)\nClick Empty to remove all data, then click Delete and confirm\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/7-feedback/","title":"Sharing &amp; Feedback","tags":[],"description":"","content":"General Evaluation 1. Working environment The working environment is friendly and open. FCJ team members are proactive and always ready to support me when I encounter difficulties, even outside working hours. The workspace is tidy and comfortable. I suggest occasional social activities or shared lunches to improve team bonding.\n2. Mentor / Admin support Mentors provide timely guidance, explain unclear points, and encourage questions. The admin team helps with documents and onboarding, enabling smooth work.\n3. Fit with academic background The assigned tasks align well with my university studies while introducing new areas I had not previously encountered, allowing me to strengthen fundamentals and gain practical skills.\n4. Learning \u0026amp; development I learned tools for project management, teamwork practices, and professional communication. Mentors shared real-world experience that helped me plan my career path.\n5. Company culture \u0026amp; teamwork The culture is positive: people respect each other and work hard while maintaining a friendly atmosphere. During tight deadlines, everyone collaborates to support the team.\n6. Internship policies \u0026amp; benefits The company offers flexible working hours and learning opportunities through workshops and events.\nAdditional questions What did you find most satisfying during the internship? Everyone is very friendly, always ready to support Suggestions \u0026amp; requests Would you like to continue in this program in the future? Yes "},{"uri":"https://roses19.github.io/fcj-workshop-roses/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://roses19.github.io/fcj-workshop-roses/tags/","title":"Tags","tags":[],"description":"","content":""}]
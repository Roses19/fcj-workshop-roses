[{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/3-blogstranslated/3.1-blog1/","title":"Xây dựng và quản lý nền tảng dữ liệu hiện đại bằng dbt và AWS Glue thông qua dbt-glue – adapter dbt mới","tags":[],"description":"","content":"Bài blog gốc: AWS Big Data Blog – dbt \u0026amp; AWS Glue\nGiới thiệu dbt là một công cụ mã nguồn mở, lấy SQL làm trung tâm, cho phép bạn viết các phép biến đổi dữ liệu có thể tái sử dụng và mở rộng bằng Python và SQL. dbt tập trung vào lớp biến đổi (transform) trong quy trình ELT/ETL trên các data warehouse và database thông qua các adapter tương ứng để đạt được khả năng trích xuất và tải dữ liệu.\nViệc này giúp data engineer, data scientist và analytics engineer định nghĩa logic nghiệp vụ bằng các câu lệnh SQL SELECT và loại bỏ nhu cầu viết thủ công các câu lệnh DML, DDL. dbt cũng giúp deploy code analytics nhanh và collaborative theo best practices của phát triển phần mềm như tính mô-đun, khả năng di chuyển, CI/CD cùng với tài liệu hóa.\ndbt được sử dụng chủ yếu với các data warehouse (như Amazon Redshift) nơi mà khách hàng muốn tách biệt logic biến đổi dữ liệu khỏi storage và engine.\nVì nhu cầu mở rộng sang cloud data lake, AWS đã giới thiệu adapter dbt-glue vào năm 2022 — một adapter AWS Glue cho dbt mã nguồn mở, đã được kiểm chứng (battle-tested), giúp data engineer dùng dbt cho cả data lake lẫn data warehouse, trả phí chỉ cho compute họ cần. Adapter dbt-glue mở rộng quyền truy cập cho người dùng dbt tới data lake, và cho phép chạy workloads biến đổi dữ liệu trên cloud với serverless Spark của AWS Glue dễ dàng. AWS tiếp tục đầu tư vào dbt-glue để hỗ trợ thêm nhiều yêu cầu khác.\nNgày nay, adapter dbt-glue đã trở thành một adapter đáng tin cậy dựa trên sự hợp tác chiến lược với dbt Labs. Adapter đáng tin cậy là adapter không do dbt Labs duy trì nhưng được dbt Labs khuyến nghị dùng ở môi trường production.\nCác tính năng chính của adapter dbt-glue:\nChạy SQL dưới dạng Spark SQL trên phiên tương tác của AWS Glue. Quản lý định nghĩa bảng trên Amazon SageMaker Lakehouse Catalog với dữ liệu lưu trên Amazon S3 Hỗ trợ các định dạng bảng mở như Apache Hudi, Delta Lake và Apache Iceberg Tương thích với AWS Lake Formation permissions để kiểm soát truy cập chi tiết Ngoài ra, dbt-glue tối ưu hóa việc sử dụng tài nguyên bằng nhiều kỹ thuật mở rộng trên các phiên tương tác của AWS Glue.\nTrường hợp sử dụng phổ biến 1. Đội analytics trong một tập đoàn\nMột đội analytics trung tâm cần theo dõi hiệu quả hoạt động. Họ ingest log ứng dụng vào các bảng Parquet ở tầng raw trong data lake trên S3. Họ cũng trích xuất dữ liệu có cấu trúc từ các hệ thống vận hành\nVí dụ: Schema tổ chức và chi phí của các thành phần khác nhau — lưu trong tầng raw dưới dạng Iceberg để giữ nguyên schema ban đầu, phục vụ cho nhiều truy cập dữ liệu.\nĐội sử dụng dbt-glue để xây dựng mô hình gold đã được transform tối ưu cho BI — kết hợp technical logs với billing data và tổ chức số liệu theo từng business unit. Iceberg hỗ trợ mô hình kiểu warehouse giúp BI phân tích perform tốt. Kết hợp Iceberg và dbt-glue cho phép xây dựng data model sẵn sàng để sử dụng.\n2. Tạo data product tuân thủ GDPR\nMột đội analytics tại công ty châu Âu có data lake trên S3 và cần tạo data product mới để làm phong phú dữ liệu y tế — yêu cầu tuân thủ GDPR như quyền bị lãng quên và xóa dữ liệu. Họ dùng Iceberg để đáp ứng những yêu cầu này, và dbt để model dữ liệu trên data lake vì dbt-glue hỗ trợ Iceberg và đơn giản hoá việc sử dụng định dạng lưu trữ này.\nCách hoạt động của dbt và dbt-glue Các tính năng chính của dbt:\nProject – cấu trúc cấp cao cho các staging, models, permissions và adapters. Có thể quản lý bằng Git. SQL – dbt dùng SQL (với templating Jinja) để biểu diễn logic biến đổi một cách modular. Thay vì phải sao chép/dán mã SQL ở nhiều nơi, các kỹ sư dữ liệu có thể định nghĩa các phép biến đổi theo mô-đun và gọi chúng từ các vị trí khác trong dự án. Việc có một quy trình xử lý dữ liệu theo mô-đun giúp các kỹ sư dữ liệu cộng tác hiệu quả hơn trên cùng một dự án. Models – mỗi dbt model là một SELECT statement lưu trong file .sql. Các kỹ sư dữ liệu định nghĩa các mô hình dbt cho cách biểu diễn dữ liệu của họ. Materializations – cách định nghĩa persist model trong đích lưu (table, view, incremental,ephemeral, and materialized view). Data lineage – theo dõi dòng dữ liệu và phân tích tác động khi thay đổi. Luồng dữ liệu tổng quan:\nData engineer ingest dữ liệu từ nguồn vào các bảng raw và định nghĩa chúng.\nViết các dbt model bằng SQL template.\ndbt adapter chuyển các dbt model thành SQL tương thích với Data Warehouse.\nHệ thống thực thi các SQL đó để tạo intermediate hoặc final tables/views/materialized views.\nSơ đồ sau mô tả kiến trúc. Cách dbt-glue hoạt động với AWS Glue\ndbt-glue chuyển dbt models sang SQL tương thích với Spark SQL.\nAWS Glue interactive sessions thực thi các SQL này để tạo bảng intermediate/final hoặc views.\ndbt-glue hỗ trợ các định dạng: csv, parquet, hudi, delta, iceberg.\nVới adapter này, materialization thường dùng là table hoặc incremental. Tùy chiến lược incremental, merge strategy yêu cầu hudi, delta, hoặc iceberg. Với append hoặc insert_overwrite, bạn có thể dùng bất kỳ định dạng kể trên.\nVí dụ minh họa Trường hợp ví dụ Trong bài này, AWS lấy dữ liệu từ dataset New York City Taxi Records trong Registry of Open Data on AWS (RODA). Dữ liệu raw Parquet lưu các thông tin chuyến đi taxi.\nMục tiêu là tạo 3 bảng metrics từ bảng raw: silver_avg_metrics – metrics cơ bản cho năm 2016 gold_passengers_metrics – metrics theo passenger gold_cost_metrics – metrics theo cost Kết quả cuối cùng là các bảng gold định dạng Iceberg để query tương tác qua Amazon Athena.\nCác bước chuẩn bị Chuẩn bị IAM Role với quyền chạy Glue interactive session và dbt-glue Tạo Glue database + table cho dataset Taxi Tạo S3 bucket để lưu dữ liệu output Cấu hình Athena để thăm dò dữ liệu Dùng CloudFormation để deploy toàn bộ hạ tầng nhanh chóng Với những bước chuẩn bị này, chúng ta mô phỏng tình huống các Data Engineers đã nhập dữ liệu từ các nguồn dữ liệu vào các bảng thô và đã định nghĩa các bảng cho các bảng thô đó. Để dễ sử dụng, chúng tôi đã chuẩn bị một mẫu CloudFormation.\nMẫu này triển khai tất cả cơ sở hạ tầng cần thiết. Để tạo các tài nguyên này, hãy chọn Khởi chạy Stack trong Vùng us-east-1.\nCác bước làm chi tiết: 1. Cài dbt và dbt CLI với dòng lệnh sau: $ pip3 install --no-cache-dir dbt-core 2. Cài đặt adapter dbt-glue với $ pip3 install --no-cache-dir dbt-glue 3. Khởi tạo dbt project (dbt init) với $ dbt init và nhập tên dbt_glue_demo. Chọn database glue. Bây giờ dự án trống đã được tạo:\n$ cd dbt_glue_demo $ tree . . ├── README.md ├── analyses ├── dbt_project.yml ├── macros ├── models │ └── example │ ├── my_first_dbt_model.sql │ ├── my_second_dbt_model.sql │ └── schema.yml ├── seeds ├── snapshots └── tests 4. Tạo source table: Trong models/ tạo file source_tables.yml với nội dung:\nversion: 2 sources: - name: data_source schema: nyctaxi tables: - name: records Source table được định nghĩa tương ứng với AWS Glue table nyctaxi.records, đây là bảng mà chúng ta đã tạo ở CloudFormation stack.\n5. Tạo models\nỞ bước này, chúng ta tạo một mô hình dbt biểu diễn các giá trị trung bình cho thời gian chuyến đi, số lượng hành khách, quãng đường chuyến đi và tổng chi phí. Hoàn thành các bước sau:\nTạo thư mục silver trong thư mục models, sau đó tạo file silver_avg_metrics.sql với nội dung sau: WITH source_avg as ( SELECT avg((CAST(dropoff_datetime as LONG) - CAST(pickup_datetime as LONG))/60) as avg_duration , avg(passenger_count) as avg_passenger_count , avg(trip_distance) as avg_trip_distance , avg(total_amount) as avg_total_amount , year , month , type FROM {{ source(\u0026#39;data_source\u0026#39;, \u0026#39;records\u0026#39;) }} WHERE year = \u0026#34;2016\u0026#34; AND dropoff_datetime is not null GROUP BY year, month, type ) SELECT * FROM source_avg Tạo file schema.yml với nội dung sau: version: 2 models: - name: silver_avg_metrics description: This table has basic metrics based on NYC Taxi Open Data for the year 2016 columns: - name: avg_duration description: The average duration of a NYC Taxi trip - name: avg_passenger_count description: The average number of passenger per NYC Taxi trip - name: avg_trip_distance description: The average NYC Taxi trip distance - name: avg_total_amount description: The avarage amount of a NYC Taxi trip - name: year description: The year of the NYC Taxi trip - name: month description: The month of the NYC Taxi trip - name: type description: The type of the NYC Taxi Tạo thư mục gold trong models, bên trong có file gold_cost_metrics.sql với nội dung sau: {{ config( materialized=\u0026#39;incremental\u0026#39;, incremental_strategy=\u0026#39;merge\u0026#39;, unique_key=[\u0026#34;year\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;type\u0026#34;], file_format=\u0026#39;iceberg\u0026#39;, iceberg_expire_snapshots=\u0026#39;False\u0026#39;, table_properties={\u0026#39;format-version\u0026#39;: \u0026#39;2\u0026#39;} ) }} SELECT (avg_total_amount/avg_trip_distance) as avg_cost_per_distance , (avg_total_amount/avg_duration) as avg_cost_per_minute , year , month , type FROM {{ ref(\u0026#39;silver_avg_metrics\u0026#39;) }} Tạo file models/gold/gold_passengers_metrics.sql với nội dung: {{ config( materialized=\u0026#39;incremental\u0026#39;, incremental_strategy=\u0026#39;merge\u0026#39;, unique_key=[\u0026#34;year\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;type\u0026#34;], file_format=\u0026#39;iceberg\u0026#39;, iceberg_expire_snapshots=\u0026#39;False\u0026#39;, table_properties={\u0026#39;format-version\u0026#39;: \u0026#39;2\u0026#39;} ) }} SELECT (avg_total_amount/avg_passenger_count) as avg_cost_per_passenger , (avg_duration/avg_passenger_count) as avg_duration_per_passenger , (avg_trip_distance/avg_passenger_count) as avg_trip_distance_per_passenger , year , month , type FROM {{ ref(\u0026#39;silver_avg_metrics\u0026#39;) }} Tạo models/gold/schema.yml với nội dung: version: 2 models: - name: gold_cost_metrics description: This table has metrics per cost based on NYC Taxi Open Data columns: - name: avg_cost_per_distance description: The average cost per distance of a NYC Taxi trip - name: avg_cost_per_minute description: The average cost per minute of a NYC Taxi trip - name: year description: The year of the NYC Taxi trip - name: month description: The month of the NYC Taxi trip - name: type description: The type of the NYC Taxi - name: gold_passengers_metrics description: This table has metrics per passenger based on NYC Taxi Open Data columns: - name: avg_cost_per_passenger description: The average cost per passenger for a NYC Taxi trip - name: avg_duration_per_passenger description: The average number of passenger per NYC Taxi trip - name: avg_trip_distance_per_passenger description: The average NYC Taxi trip distance - name: year description: The year of the NYC Taxi trip - name: month description: The month of the NYC Taxi trip - name: type description: The type of the NYC Taxi Cuối cùng, xóa models/example/ vì đây là folder ví dụ khi chạy dbt init. 6. Cấu hình dbt project\nFile dbt_project.yml bao gồm nội dung: models: dbt_glue_demo: # Config indicated by + and applies to all files under models/example/ example: +materialized: view Vì ta muốn hiện thực hóa các mô hình dưới lớp silver dưới dạng Parquet. ta cần thay thế nội dung trong file dbt_project.yml như sau: models: dbt_glue_demo: silver: +materialized: table 7. Cấu hình dbt profile\ndbt profile là một cấu hình quy định cách kết nối với một cơ sở dữ liệu cụ thể và được định nghĩa trong tệp profiles.yml trong dự án dbt.\nThực hiện các bước sau để cấu hình dbt profile:\nTạo thư mục profiles.\nTạo tệp profiles/profiles.yml với nội dung sau:\ndbt_glue_demo: target: dev outputs: dev: type: glue query-comment: demo-nyctaxi role_arn: \u0026#34;{{ env_var(\u0026#39;DBT_ROLE_ARN\u0026#39;) }}\u0026#34; region: us-east-1 workers: 5 worker_type: G.1X schema: \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34; database: \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34; session_provisioning_timeout_in_seconds: 120 location: \u0026#34;{{ env_var(\u0026#39;DBT_S3_LOCATION\u0026#39;) }}\u0026#34; Tạo thư mục profiles/iceberg/\nTạo file profiles/iceberg/profiles.yml với nội dung:\ndbt_glue_demo: target: dev outputs: dev: type: glue query-comment: demo-nyctaxi role_arn: \u0026#34;{{ env_var(\u0026#39;DBT_ROLE_ARN\u0026#39;) }}\u0026#34; region: us-east-1 workers: 5 worker_type: G.1X schema: \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34; database: \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34; session_provisioning_timeout_in_seconds: 120 location: \u0026#34;{{ env_var(\u0026#39;DBT_S3_LOCATION\u0026#39;) }}\u0026#34; datalake_formats: \u0026#34;iceberg\u0026#34; conf: spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql catalog.glue_catalog.warehouse={{ env_var(\u0026#39;DBT_S3_LOCATION\u0026#39;) }}warehouse/ --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions Hai dòng cuối cùng được thêm vào để thiết lập cấu hình Iceberg trên các phiên tương tác AWS Glue.\n8. Chạy dbt project\nBây giờ là lúc chạy dự án dbt. Hoàn thành các bước sau:\nĐể chạy project dbt, bạn phải ở trong thư mục dự án: ``$ cd dbt_glue_demo``` Dự án yêu cầu bạn đặt các biến môi trường để chạy trên tài khoản AWS: $ export DBT_ROLE_ARN=\u0026#34;arn:aws:iam::$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text):role/GlueInteractiveSessionRole\u0026#34; $ export DBT_S3_LOCATION=\u0026#34;s3://aws-dbt-glue-datalake-$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text)-us-east-1\u0026#34; Đảm bảo cấu hình được thiết lập chính xác từ dòng lệnh: $ dbt debug --profiles-dir profiles ... 05:34:22 Connection test: [OK connection ok] 05:34:22 All checks passed! Nếu bạn gặp bất kỳ lỗi nào, hãy kiểm tra xem bạn đã cung cấp đúng IAM role ARN và vị trí S3 ở Bước 2 chưa. 4. Chạy các mô hình với đoạn mã sau:\n$ dbt run -m silver --profiles-dir profiles $ dbt run -m gold --profiles-dir profiles/iceberg/ Giờ đây, các bảng đã được tạo thành công trong SageMaker Lakehouse Catalog và dữ liệu được hiện thực hóa ở Amazon S3 location.\nBạn có thể xác minh các bảng đó bằng cách mở bảng điều khiển AWS Glue, chọn Databases trong ngăn điều hướng và mở dbt_glue_demo_nyc_metrics. Truy vấn các bảng được cụ thể hóa thông qua Athena Hãy truy vấn bảng mục tiêu bằng cách sử dụng Athena để xác minh các bảng được cụ thể hóa. Hoàn thành các bước sau:\nTrên bảng điều khiển Athena, chuyển nhóm làm việc sang athena-dbt-glue-aws-blog.\nNếu hộp thoại cài đặt athena-dbt-glue-aws-blog của nhóm làm việc xuất hiện, hãy chọn Acknowledge.\nSử dụng truy vấn sau để khám phá các số liệu được tạo bởi dự án dbt:\nSELECT cm.avg_cost_per_minute , cm.avg_cost_per_distance , pm.avg_cost_per_passenger , cm.year , cm.month , cm.type FROM \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34;.\u0026#34;gold_passengers_metrics\u0026#34; pm LEFT JOIN \u0026#34;dbt_glue_demo_nyc_metrics\u0026#34;.\u0026#34;gold_cost_metrics\u0026#34; cm ON cm.type = pm.type AND cm.year = pm.year AND cm.month = pm.month WHERE cm.type = \u0026#39;yellow\u0026#39; AND cm.year = \u0026#39;2016\u0026#39; AND cm.month = \u0026#39;6\u0026#39; Ảnh chụp màn hình sau đây hiển thị kết quả của truy vấn này. Xem lại tài liệu dbt Hãy hoàn tất các bước sau để xem lại tài liệu của bạn:\nTạo các tài liệu sau cho dự án: $ dbt docs generate --profiles-dir profiles/iceberg 11:41:51 Running with dbt=1.7.1 11:41:51 Registered adapter: glue=1.7.1 11:41:51 Unable to do partial parsing because profile has changed 11:41:52 Found 3 models, 1 source, 0 exposures, 0 metrics, 478 macros, 0 groups, 0 semantic models 11:41:52 11:41:53 Concurrency: 1 threads (target=\u0026#39;dev\u0026#39;) 11:41:53 11:41:53 Building catalog 11:43:32 Catalog written to /Users/username/Documents/workspace/dbt_glue_demo/target/catalog.json Chạy lệnh sau để mở tài liệu trên trình duyệt của bạn: $ dbt docs serve --profiles-dir profiles/iceberg Trong ngăn điều hướng, chọn gold_cost_metrics trong dbt_glue_demo/models/gold. Bạn có thể xem chế độ xem chi tiết của mô hình gold_cost_metrics, như minh họa trong ảnh chụp màn hình sau. Để xem biểu đồ dòng, hãy chọn biểu tượng hình tròn ở dưới cùng bên phải.\nDọn dẹp Để làm sạch môi trường của bạn, hãy hoàn thành các bước sau:\nXóa cơ sở dữ liệu được tạo bởi dbt: $ aws glue delete-database —name dbt_glue_demo_nyc_metrics\nXóa tất cả dữ liệu đã tạo:\n$ aws s3 rm s3://aws-dbt-glue-datalake-$(aws sts get-caller-identity —query \u0026#34;Account\u0026#34; —output text)-us-east-1/ —recursive $ aws s3 rm s3://aws-athena-dbt-glue-query-results-$(aws sts get-caller-identity —query \u0026#34;Account\u0026#34; —output text)-us-east-1/ —recursive Xóa ngăn xếp CloudFormation: $ aws cloudformation delete-stack —stack-name dbt-demo Kết luận Bài viết này đã trình bày cách bộ chuyển đổi dbt-glue hỗ trợ khối lượng công việc của bạn và cách bạn có thể xây dựng một ngăn xếp dữ liệu hiện đại bằng dbt và AWS Glue sử dụng bộ chuyển đổi dbt-glue. Bạn đã học được các thao tác từ đầu đến cuối và luồng dữ liệu để các kỹ sư dữ liệu xây dựng và quản lý ngăn xếp dữ liệu bằng dbt và bộ chuyển đổi dbt-glue.\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/","title":"Báo cáo thực tập","tags":[],"description":"","content":"Báo cáo thực tập Thông tin sinh viên: Họ và tên: Nguyễn Thị Ánh Hồng\nSố điện thoại: 0327761048\nEmail: 2251052039hong@ou.edu.vn\nTrường: Đại học Mở TP.HCM\nNgành: Công nghệ thông tin\nLớp: DH22IT02\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Data Intern\nThời gian thực tập: Từ ngày 15/12/2025 đến ngày 17/1/2026\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/5.3-ingestion/5.3.1-docker-configuration/","title":"Cấu hình Docker ","tags":[],"description":"","content":"1 . Tạo Airflow Image tùy chỉnh\nXây dựng Dockerfile: FROM apache/airflow:2.8.4-python3.10 USER root RUN apt-get update \u0026amp;\u0026amp; apt-get install -y gcc python3-dev USER airflow COPY requirements.txt /opt/airflow/requirements.txt RUN pip install --no-cache-dir -r /opt/airflow/requirements.txt Một Docker image riêng được xây dựng từ Dockerfile: apache/airflow:2.8.4-python3.10\nImage này đảm bảo mọi container Airflow có cùng môi trường runtime.\n2 . Định nghĩa cấu hình dùng chung Một block cấu hình chung (x-airflow-common) được tạo ra để:\nDùng chung image. Dùng chung biến môi trường. Mount các thư mục config, dags, data, etls, logs, pipelines, plugins, tests, utils. x-airflow-common: \u0026amp;airflow-common build: context: . dockerfile: Dockerfile image: custom-airflow:2.8.4-python3.10 env_file: - airflow.env volumes: - ./config:/opt/airflow/config - ./dags:/opt/airflow/dags - ./data:/opt/airflow/data - ./etls:/opt/airflow/etls - ./logs:/opt/airflow/logs - ./pipelines:/opt/airflow/pipelines - ./plugins:/opt/airflow/plugins - ./tests:/opt/airflow/tests - ./utils:/opt/airflow/utils - ./requirements.txt:/opt/airflow/requirements.txt depends_on: - postgres - redis 3 . Triển khai các dịch vụ:\nPostgreSQL để lưu DAG, trạng thái task, lịch sử chạy: postgres: image: postgres:12 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: postgres POSTGRES_DB: airflow_reddit ports: - \u0026#34;5432:5432\u0026#34; Thiết lập Message Broker (Redis) để nhiều Worker xử lý song song và Retry Task khi bị lỗi redis: image: redis:latest ports: - \u0026#34;6379:6379\u0026#34; Khởi tạo hệ thống (airflow-init): Tạo schema database, tài khoản admin airflow-init: \u0026lt;\u0026lt;: *airflow-common command: \u0026gt; bash -c \u0026#34; pip install -r /opt/airflow/requirements.txt \u0026amp;\u0026amp; airflow db migrate \u0026amp;\u0026amp; airflow db upgrade \u0026amp;\u0026amp; airflow db check \u0026amp;\u0026amp; airflow users create --username admin --firstname admin --lastname admin --role Admin --email nguyenthihong112000@gmail.com --password admin\u0026#34; restart: \u0026#34;no\u0026#34; Sau khi hoàn tất, hệ thống Airflow sẵn sàng hoạt động. Triển khai các thành phần Airflow Webserver cung cấp giao diện để quản lý DAG, theo dõi trạng thái và xem log. airflow-webserver: \u0026lt;\u0026lt;: *airflow-common command: airflow webserver ports: - \u0026#34;8080:8080\u0026#34; Scheduler chịu trách nhiệm lập lịch và gửi task vào hàng đợi để thực thi. airflow-scheduler: \u0026lt;\u0026lt;: *airflow-common command: airflow scheduler Worker (Celery) thực hiện các task như gọi Reddit API và ghi dữ liệu vào S3. airflow-worker: \u0026lt;\u0026lt;: *airflow-common container_name: airflow-worker command: airflow celery worker ports: - \u0026#34;8793:8793\u0026#34; Triggerer xử lý các tác vụ bất đồng bộ và sensor. airflow-triggerer: \u0026lt;\u0026lt;: *airflow-common command: airflow triggerer "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/5.1-workshop-overview/","title":"Giới thiệu","tags":[],"description":"","content":"Giới thiệu về Data Platform Data Platform là hệ thống cho phép thu thập, lưu trữ, xử lý và phân tích dữ liệu ở quy mô lớn một cách tự động và có khả năng mở rộng. Trong kiến trúc này, dữ liệu được tổ chức theo các tầng (raw, transformed, analytics) nhằm đảm bảo dữ liệu gốc được bảo toàn, đồng thời tối ưu cho việc phân tích và báo cáo.\nTổng quan về workshop Trong workshop này, dữ liệu Reddit được phân tích trên AWS dựa trên kiến trúc Data Lake kết hợp Data Warehouse.\nHệ thống bao gồm:\n\u0026ldquo;Apache Airflow\u0026rdquo; chạy trong Docker để thu thập dữ liệu từ Reddit API và điều phối pipeline. \u0026ldquo;Amazon S3\u0026rdquo; để lưu trữ dữ liệu ở hai lớp: Raw và Transformed. \u0026ldquo;AWS Glue \u0026amp; Data Catalog\u0026rdquo; để tự động phát hiện schema và xử lý dữ liệu. \u0026ldquo;Amazon Athena\u0026rdquo; để truy vấn dữ liệu trực tiếp trên S3. \u0026ldquo;Amazon Redshift\u0026rdquo; để lưu trữ dữ liệu phân tích. \u0026ldquo;Amazon QuickSight\u0026rdquo; để xây dựng dashboard và trực quan hóa dữ liệu. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/5.4-data-catalog/5.4.1--data-lake/","title":"Lưu trữ vào Data Lake (Amazon S3)","tags":[],"description":"","content":"Để lưu trữ dữ liệu một cách bền vững, bạn sẽ cần phải phân tầng lưu trữ tại S3\nThực hiện kết nối với bucket, với các giá trị AWS_ACCESS_KEY_ID, AWS_SECRET_KEY được cấu hình riêng tại file constants.py, đảm bảo bucket tồn tại và ghi dữ liệu vào bucket: def connect_to_s3(): try: s3= s3fs.S3FileSystem(anon=False, key=AWS_ACCESS_KEY_ID, secret=AWS_SECRET_KEY) return s3 except Exception as e: print(e) def create_bucket_if_not_exist(s3:s3fs.S3FileSystem,bucket:str): try: if not s3.exists(bucket): s3.mkdir(bucket) print(\u0026#34;Bucket created\u0026#34;) else: print(\u0026#34;Bucket already exists\u0026#34;) except Exception as e: print(\u0026#34;Error here\u0026#34;) print(e) def upload_to_s3(s3:s3fs.S3FileSystem,bucket:str,file_path: str,s3_file_name: str): try: s3.put(file_path,bucket+\u0026#39;/raw/\u0026#39;+ s3_file_name) print(\u0026#34;File upload to s3\u0026#34;) except FileNotFoundError: print(\u0026#34;The file was not found\u0026#34;) Viết task Airflow để upload dữ liệu lên S3: upload_s3 = PythonOperator( task_id = \u0026#39;s3_upload\u0026#39;, python_callable = upload_s3_pipeline, dag=dag ) extract \u0026gt;\u0026gt; upload_s3 Với python_callable là upload_s3_pipeline: def upload_s3_pipeline(ti): file_path = ti.xcom_pull(task_ids= \u0026#39;reddit_extraction\u0026#39;,key = \u0026#39;return_value\u0026#39;) s3 =connect_to_s3() create_bucket_if_not_exist(s3, AWS_BUCKET_NAME) upload_to_s3(s3, AWS_BUCKET_NAME,file_path, file_path.split(\u0026#39;/\u0026#39;)[-1]) "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/1-worklog/","title":"Nhật ký công việc","tags":[],"description":"","content":"Chào mừng bạn đến với trang Nhật ký công việc của tôi. Đây là nơi ghi lại chi tiết lộ trình 5 tuần thực hiện dự án Reddit Data Pipeline và tìm hiểu các dịch vụ trên AWS.\nTại đây sẽ được chia sẻ các cột mốc từ việc thiết lập hạ tầng Cloud bảo mật , xây dựng đường ống dữ liệu tự động cho đến khâu trực quan hóa báo cáo.\nCác công việc hằng tuần được tóm tắt như sau:\nTuần 1 Thu thập dữ liệu Reddit\nTuần 2 Xử lý \u0026amp; lưu trữ dữ liệu\nTuần 3 Phân tích \u0026amp; trực quan hóa\nTuần 4 Tự động hóa, giám sát \u0026amp; đánh giá hệ thống\nTuần 5 Báo cáo \u0026amp; định hướng mở rộng\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/1-worklog/1.1-week1/","title":"Worklog Tuần 1","tags":[],"description":"","content":"Mục tiêu tuần 1: Thành thạo dịch vụ lưu trữ Amazon S3, các cơ chế bảo mật và tối ưu hóa phân phối qua CloudFront. Thiết lập hạ tầng cốt lõi và phân quyền bảo mật cho dự án Reddit Data Pipeline (Airflow, S3, IAM). Xây dựng và vận hành thành công luồng ETL tự động để lấy dữ liệu từ Reddit về Data Lake. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tìm hiểu Amazon S3 Essentials. - Thực hành Static Website Hosting: + Cấu hình Static Website Hosting, thiết lập Index \u0026amp; Error document. + Quản lý quyền truy cập: Block Public Access, IAM Policy và ACLs. 15/12/2025 15/12/2025 https://000057.awsstudygroup.com/ 3 - Quản lý dữ liệu \u0026amp; Tối ưu hóa: + Kích hoạt Versioning để bảo vệ dữ liệu khỏi việc xóa nhầm. + Thiết lập CRR để dự phòng thảm họa. + Tăng tốc website với CDN và cấu hình OAC. 16/12/2025 16/12/2025 https://000057.awsstudygroup.com/vi/7-cloudfront/ 4 - Khởi tạo hạ tầng \u0026amp; Bảo mật cho dự án: + Thiết lập Airflow.\n+ Tạo S3 Buckets. + Thiết lập IAM Roles/Policies cho Airflow, S3 và Athena. + Đăng ký Reddit App để lấy Client ID/Secret. 17/12/2025 18/12/2025 5 - Tìm hiểu dịch vụ AWS Glue, Athena. - Thực hành: + Tạo Role, policy phù hợp. + Tạo S3 bucket, ETL jobs, crawler. 18/12/2025 18/12/2025 https://000035.awsstudygroup.com/vi/ 6 - Data Discovery (Glue \u0026amp; Athena): + Chạy Glue Crawler để quét schema dữ liệu thô trên S3. + Dùng Athena thực hiện truy vấn SQL kiểm tra dữ liệu vừa đẩy lên. - Phát triển ETL Pipeline: + Viết Airflow DAG để crawl dữ liệu từ Reddit. + Thực hiện đẩy dữ liệu thô lên S3. 19/12/2025 19/12/2025 Kết quả đạt được tuần 1: Quản trị lưu trữ và Phân phối nội dung:\nNắm vững kiến trúc Amazon S3 và triển khai thành công Static Website Hosting với đầy đủ cấu hình Index/Error document. Thiết lập các lớp bảo mật nghiêm ngặt thông qua Block Public Access và Bucket Policy; bảo vệ dữ liệu bằng Versioning và sao chép xuyên vùng (CRR). Tối ưu hóa hiệu suất website bằng cách tích hợp CloudFront CDN và sử dụng Origin Access Control (OAC) để bảo mật nguồn dữ liệu. Thiết lập hạ tầng dự án Reddit:\nKhởi tạo thành công môi trường điều phối Apache Airflow và đăng ký định danh Reddit API (Client ID/Secret). Xây dựng cấu trúc lưu trữ phân tầng trên S3 (Raw, Transformed, Athena) bám sát mô hình kiến trúc Data Lake. Cấu hình hệ thống IAM Roles/Policies chuyên biệt cho Airflow và Athena, tuân thủ nguyên tắc đặc quyền tối thiểu (Least Privilege). Data Discovery và Luồng ETL:\nVận hành AWS Glue Crawler để tự động quét schema và cập nhật Metadata cho dữ liệu thô trên S3. Thực hiện thành công các truy vấn SQL ad-hoc trên Amazon Athena để kiểm tra tính toàn vẹn của dữ liệu vừa đẩy lên. Phát triển hoàn chỉnh Airflow DAG đầu tiên, tự động hóa quy trình crawl dữ liệu từ Reddit và nạp vào vùng Raw của Data Lake. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/1-worklog/1.2-week2/","title":"Worklog Tuần 2","tags":[],"description":"","content":"Mục tiêu tuần 2: Nắm vững cách xử lý, làm sạch và tối ưu dữ liệu bằng AWS Glue và PySpark. Hiểu và áp dụng Monitoring, Cost Management và Security cho hệ thống Data Platform. Tìm hiểu các dịch vụ AWS Data \u0026amp; Analytics gồm S3, Glue, Athena và QuickSight. Xây dựng và quản trị hệ thống Data Lake và quy trình ETL trên AWS. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Viết ETL Transformation:\n+ Xây dựng Job Glue (Pyspark) để làm sạch dữ liệu. + Chuyển đổi định dạng để tối ưu hiệu suất. 22/12/2025 22/12/2025 3 - Tìm hiểu Monitoring \u0026amp; Cost Management + CloudWatch logs \u0026amp; metrics + AWS Budgets + Cost Explorer 23/12/2025 23/12/2025 https://000008.awsstudygroup.com/ 4 - Tìm hiểu IAM \u0026amp; Security cho Data Platform + IAM User vs Role vs Policy + IAM cho Glue, Athena, Redshift + S3 Bucket Policy vs IAM Policy + KMS \u0026amp; mã hóa dữ liệu 24/12/2025 24/12/2025 https://000033.awsstudygroup.com/ 5 - Tìm hiểu Networking cho AWS Data Services + VPC, Subnet, Internet Gateway + VPC Endpoint cho S3 \u0026amp; Glue + Public vs Private access + Data services giao tiếp với nhau thế nào 25/12/2025 26/12/2025 https://000003.awsstudygroup.com/vi/ 6 - Xây dựng Data Lake trên AWS: Tìm hiểu các dịch vụ AWS Glue, Athena, Quicksight. - Thực hành: + Tạo Role, policy phù hợp. + Tạo S3 bucket, ETL jobs, crawler + Truy vấn SQL với Athena. 26/12/2025 26/12/2025 https://000035.awsstudygroup.com/vi Kết quả đạt được tuần 2: Đã xây dựng ETL Job bằng AWS Glue (PySpark) để:\nLàm sạch dữ liệu Reddit\nChuẩn hóa schema cho phân tích\nChuyển dữ liệu từ Raw S3 → Processed S3\nĐã thiết lập Monitoring \u0026amp; Cost Management:\nBật CloudWatch Logs cho Glue Jobs\nThiết lập AWS Budgets để theo dõi chi phí\nSử dụng Cost Explorer để phân tích usage\nĐã cấu hình Security cho Data Platform:\nIAM Roles cho Glue, Athena, S3\nS3 Bucket Policy theo nguyên tắc least privilege\nBật KMS encryption cho dữ liệu\nĐã thiết kế và hiểu rõ Networking cho AWS Data Services:\nVPC, Subnet, Internet Gateway\nVPC Endpoint cho S3 \u0026amp; Glue\nPhân biệt Public vs Private access\nĐã xây dựng AWS Data Lake:\nTạo S3 structure: raw/, processed/\nChạy Glue Crawler để tạo Data Catalog\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/1-worklog/1.3-week3/","title":"Worklog Tuần 3","tags":[],"description":"","content":"Mục tiêu tuần 3: Triển khai kho dữ liệu và phân tích hiệu năng cao với Redshift Serverless. Thiết kế bảng điều khiển BI tương tác chuyên nghiệp qua Amazon QuickSight. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 Thiết lập \u0026amp; làm quen Amazon Redshift + Tổng quan Data Warehouse \u0026amp; Redshift Serverless + Tạo Namespace \u0026amp; Workgroup + Cấu hình IAM Role truy cập S3 29/12/2025 30/12/2025 https://docs.aws.amazon.com/redshift/ 3 - Nạp \u0026amp; phân tích dữ liệu trong Redshift + Tạo bảng (Schema, Tables) + Sử dụng lệnh COPY load dữ liệu từ S3 + Tạo Table và thực hiện lệnh COPY để load dữ liệu từ S3. + Viết SQL phân tích bằng Query Editor v2 30/12/2025 30/12/2025 https://docs.aws.amazon.com/redshift/ 4 - Tìm hiểu Amazon QuickSight, các thành phần cốt lõi: SPICE, Dataset, Analysis và Dashboard - Thực hành: + Chuẩn bị dữ liệu và thiết lập môi trường tại Region Singapore + Tạo các biểu đồ trực quan + Tối ưu giao diện và thiết lập tính tương tác + Xuất bản Dashboard và dọn dẹp tài nguyên 31/12/2025 31/12/2025 https://000073.awsstudygroup.com/ Kết quả đạt được tuần 3: AWS Redshift Serverless:\nKhởi tạo thành công môi trường Namespace/Workgroup và nạp dữ liệu từ S3 vào bảng bằng lệnh COPY. Sử dụng Query Editor v2 để thực hiện các phân tích SQL chuyên sâu trên kho dữ liệu. Amazon QuickSight:\nTối ưu hóa hiệu suất xử lý dữ liệu nhờ công cụ tính toán trong bộ nhớ SPICE. Triển khai đa dạng các loại hình trực quan hóa: Line chart, KPI \u0026amp; Insights, Pie chart và Pivot Table. Thiết lập tính tương tác linh hoạt cho báo cáo thông qua bộ lọc (Filters) và các hành động điều hướng (Navigation Actions). Xuất bản Dashboard thành công và thực hiện quy trình dọn dẹp tài nguyên để tối ưu hóa chi phí vận hành. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/1-worklog/1.4-week4/","title":"Worklog Tuần 4","tags":[],"description":"","content":"Mục tiêu tuần 4: Mở rộng kiến thức về kiến trúc Data Platform trên AWS. Tìm hiểu cách các hệ thống dữ liệu lớn được tự động hóa, giám sát và vận hành. Thực hiện các lab và bài thực hành để làm quen với các dịch vụ nâng cao trên AWS. Đánh giá kiến trúc và chi phí của hệ thống Reddit Data Platform hiện tại.. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2,3 Tìm hiểu kiến trúc tự động hóa dữ liệu + Event-driven vs Scheduled pipelines + AWS Lambda \u0026amp; EventBridge + So sánh với Airflow 05/01/2026 06/01/2026 https://docs.aws.amazon.com/eventbridge/ 4 Thực hành: Monitoring \u0026amp; Observability + CloudWatch logs \u0026amp; metrics + Dashboard \u0026amp; Alert + SLA cho pipeline 07/01/2026 07/01/2026 https://000008.awsstudygroup.com/vi/ 5 Rà soát kiến trúc hệ thống + Kiểm tra luồng dữ liệu Reddit → S3 → Glue → Athena → Redshift → QuickSight + Xác định bottleneck và rủi ro 08/01/2026 08/01/2026 6 Phân tích chi phí \u0026amp; hiệu năng + Glue, S3, Athena, Redshift, QuickSight + Đề xuất phương án tối ưu 09/01/2026 09/01/2026 Kết quả đạt được tuần 4: Hiểu rõ các mô hình tự động hóa pipeline (event-driven và scheduled) và cách áp dụng trong hệ thống dữ liệu. Thực hành monitoring, logging và alerting cho Data Platform bằng CloudWatch. Đánh giá được kiến trúc tổng thể của Reddit Data Platform và các điểm nghẽn tiềm năng. Phân tích chi phí và hiệu năng của các dịch vụ AWS đang sử dụng (Glue, S3, Athena, Redshift, QuickSight). Xác định được các hướng tối ưu và mở rộng cho giai đoạn tiếp theo. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/1-worklog/1.5-week5/","title":"Worklog Tuần 5","tags":[],"description":"","content":"Mục tiêu tuần 5: Hoàn thiện báo cáo và tổng kết dự án Reddit Data Platform. Đánh giá kiến trúc, hiệu năng và chi phí của hệ thống hiện tại. Tìm hiểu các mô hình Data Platform nâng cao để định hướng mở rộng trong tương lai. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 Viết báo cáo kiến trúc \u0026amp; luồng dữ liệu + Reddit → S3 → Glue → Athena → Redshift → QuickSight + Vẽ sơ đồ kiến trúc 12/01/2026 12/01/2026 3 Tổng kết \u0026amp; đánh giá hệ thống + Đánh giá hiệu năng + Phân tích chi phí + Tổng hợp kết quả 13/01/2026 13/01/2026 4 Tìm hiểu kiến trúc Data Platform nâng cao + Batch vs Streaming + Kinesis \u0026amp; Firehose + Data Lakehouse 14/01/2026 14/01/2026 https://000072.awsstudygroup.com/vi/10-kinesis-data-analytics/ 5 Tìm hiểu mở rộng hệ thống 15/01/2026 15/01/2026 6 Định hướng phát triển \u0026amp; roadmap + Xây dựng kiến trúc tương lai + Đề xuất cải tiến + Hướng phát triển project 16/01/2026 16/01/2026 Kết quả đạt được tuần 5: Hoàn thiện báo cáo kỹ thuật mô tả đầy đủ Reddit Data Platform. Tổng kết và đánh giá chi phí, hiệu năng và kiến trúc của hệ thống. Xác định hạn chế và điểm cần cải thiện của pipeline hiện tại. Nắm được các mô hình Data Platform hiện đại như streaming, lakehouse và multi-source ingestion. Xây dựng roadmap mở rộng cho hệ thống trong tương lai. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Bài thu hoạch “AWS FIRST CLOUD JOURNEY COMMUNITY DAY 2025” Mục Đích Của Sự Kiện Danh Sách Diễn Giả Nội Dung Nổi Bật Đưa ra các ảnh hưởng tiêu cực của kiến trúc ứng dụng cũ "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/2-proposal/","title":"Bản đề xuất","tags":[],"description":"","content":"Automating the ETL Pipeline for Reddit Data on AWS Tự động hóa pipeline ETL cho dữ liệu Reddit trên AWS 1. Tóm tắt điều hành Dự án được thiết kế nhằm xây dựng một hệ thống thu thập và xử lý dữ liệu mạng xã hội tự động, tối ưu hóa giữa tài nguyên có sẵn và hạ tầng đám mây. Hệ thống áp dụng mô hình kiến trúc lai (Hybrid): sử dụng Apache Airflow chạy trên máy chủ cục bộ vật lý (local server) để điều phối và các dịch vụ AWS Serverless để xử lý và lưu trữ dữ liệu quy mô lớn.\nQuy trình này giải quyết bài toán tự động hóa hoàn toàn vòng đời dữ liệu Reddit: từ khâu trích xuất (Ingestion), chuẩn hóa schema tự động bằng Glue Crawler, đến biến đổi dữ liệu Spark ETL sang định dạng Parquet giúp tiết kiệm chi phí truy vấn đáng kể.3 Hệ thống tận dụng sức mạnh của Amazon Redshift Serverless với cấu hình tối thiểu (4 RPU) để thực hiện phân tích hiệu năng cao và trình diễn qua Amazon QuickSight Standard, giúp giảm thiểu tối đa chi phí vận hành hàng tháng mà vẫn đảm bảo khả năng xử lý dữ liệu linh hoạt.\n2. Tuyên bố vấn đề Vấn đề hiện tại\nQuy trình khai thác dữ liệu từ các nền tảng mạng xã hội hiện nay đối mặt với nhiều thách thức :\nHạ tầng rời rạc: Việc cào dữ liệu (scraping) và phân tích thường không được kết nối, dẫn đến dữ liệu bị cũ hoặc mất mát khi thực hiện thủ công. Lãng phí tài nguyên tính toán: Duy trì một kho dữ liệu (24/7) cho các truy vấn không thường xuyên gây tốn kém ngân sách lớn, không phù hợp với các dự án vừa và nhỏ. Chi phí API và Rate Limit: Reddit áp dụng các hạn chế nghiêm ngặt về tốc độ truy cập (100 QPM), đòi hỏi hệ thống điều phối phải có cơ chế quản lý thông minh để tránh bị khóa tài khoản. Giải pháp\nXây dựng một đường ống dữ liệu hiện đại hóa :\nLocal Orchestration: Tận dụng phần cứng có sẵn để chạy Airflow trong Docker, giảm chi phí thuê máy chủ cloud cho khâu điều phối. Serverless Ingestion \u0026amp; ETL: Sử dụng Amazon S3 và AWS Glue để xử lý dữ liệu phi cấu trúc khối lượng lớn một cách linh hoạt mà không cần quản lý máy chủ. Optimized Analytics: Triển khai Redshift Serverless cấu hình $4$ RPU để thực hiện SQL analytics chỉ khi có yêu cầu, đảm bảo hiệu năng cao với chi phí cực thấp. Lợi ích và hoàn vốn đầu tư (ROI)\nGiải pháp Hybrid giúp cắt giảm khoảng 30-40% chi phí cố định so với việc chạy toàn bộ trên cloud. Chi phí vận hành thực tế chỉ phát sinh khi dữ liệu được xử lý trên AWS (khoảng $25 - $30/tháng). Thời gian từ lúc có tin tức nóng hổi trên Reddit đến khi xuất hiện trên báo cáo của QuickSight được rút ngắn xuống tính bằng giờ, thay vì hàng ngày.17\n3. Kiến trúc giải pháp Hệ thống áp dụng kiến trúc hướng sự kiện (Event-driven) trên nền tảng Serverless, kết hợp với bộ điều phối Container chạy tại máy chủ vật lý cục bộ.\nDịch vụ AWS sử dụng\nAWS S3: Đóng vai trò là Data Lake, lưu trữ dữ liệu thô (Raw) và dữ liệu đã qua xử lý (Transformed). AWS Glue Crawler: Tự động quét các tập tin trên S3 để nhận diện cấu trúc (schema) và cập nhật metadata. AWS Glue Data Catalog: Kho lưu trữ metadata trung tâm, cho phép các dịch vụ phân tích truy cập dữ liệu S3 một cách thống nhất. AWS Glue ETL: Dịch vụ xử lý dữ liệu Spark serverless, dùng để chuyển đổi định dạng JSON sang Parquet và làm sạch dữ liệu. Amazon Athena: Dịch vụ truy vấn SQL serverless trực tiếp trên S3 phục vụ các phân tích tức thời. Amazon Redshift Serverless: Kho dữ liệu (Data Warehouse) với khả năng tự động co giãn, thực hiện các truy vấn phân tích hiệu năng cao. Amazon QuickSight Standard: Nền tảng BI serverless để xây dựng các Dashboard trực quan hóa. Thiết kế thành phần\nLuồng dữ liệu được thực hiện qua 3 giai đoạn chính:\nTrích xuất: Airflow (Local Docker) gọi Reddit API và lưu dữ liệu JSON thô vào Amazon S3. Xử lý: AWS Glue Crawler cập nhật schema , sau đó Glue Spark ETL chuẩn hóa và chuyển đổi dữ liệu sang định dạng Parquet lưu tại S3. Phân tích: Dữ liệu từ S3 được nạp vào Redshift Serverless và hiển thị trực quan qua QuickSight Dashboard. 4. Triển khai kỹ thuật Các giai đoạn triển khai\nThiết lập Local Environment: Cấu hình Docker Compose cho Airflow. Thiết lập kết nối an toàn từ máy chủ cục bộ lên AWS thông qua IAM User/Access Key. Phát triển Reddit Extractor: Viết script Python sử dụng thư viện PRAW. Triển khai logic \u0026ldquo;Exponential Backoff\u0026rdquo; để tuân thủ giới hạn API của Reddit.ư Xây dựng Glue Pipeline: Thiết lập Crawler quét S3 và viết job Spark ETL để chuyển đổi, chuẩn hóa dữ liệu Reddit sang Parquet phân vùng theo ngày. Thiết lập Kho dữ liệu: Khởi tạo Redshift Serverless với namespace và workgroup, cấu hình mức 4 RPU cơ bản. Thiết kế Dashboard: Kết nối QuickSight Standard tới Redshift để tạo các biểu đồ theo dõi từ khóa và xu hướng thảo luận. Yêu cầu kỹ thuật\nCấu hình Reddit API: Sử dụng tài khoản Developer loại \u0026ldquo;script\u0026rdquo; để thu thập dữ liệu đảm bảo tuân thủ giới hạn 100 QPM của Reddit. Hạ tầng điều phối (Compute): Máy chủ vật lý cục bộ chạy Docker/Airflow cần cấu hình tối thiểu 8 GB RAM để vận hành ổn định các worker Celery. Lưu trữ (Storage): Sử dụng Amazon S3 với định dạng Parquet, giúp giảm tới 90% dung lượng lưu trữ và chi phí quét dữ liệu so với JSON thô. Amazon Redshift Serverless: Thiết lập mức dung lượng tối thiểu 4 RPU để tối ưu chi phí cho các tác vụ phân tích vừa và nhỏ. Amazon QuickSight Standard: Sử dụng phiên bản Standard để tạo các báo cáo BI cơ bản với chi phí cố định thấp nhất. 5. Lộ trình \u0026amp; Mốc triển khai Trước thực tập (Tháng 0): 1 tháng lên kế hoạch và học tập các dịch vụ cơ bản AWS, kiến trúc ETL. Thực tập: Tuần 1-2: Học AWS và nâng cấp phần cứng. Tuần 3: Thiết kế và điều chỉnh kiến trúc. Tuần 4-5: Triển khai, kiểm thử, đưa vào sử dụng. Sau triển khai: Nghiên cứu thêm trong vòng 6 tháng để nâng cấp hệ thống. 6. Ước tính ngân sách Có thể xem chi phí trên AWS Pricing Calculator\nChi phí hạ tầng\nS3 Standard: 0,17 USD/tháng (6 GB, 5000 request, 1 GB quét). Truyền dữ liệu: 0,54 USD/tháng (6 GB vào, 6 GB ra). AWS Glue ETL Jobs: 3,09 USD/tháng (2 DPU, ETL Spark jobs và Python Shell jobs). AWS Glue Crawlers: 0,66 USD/tháng (1 crawler). AWS Glue Data Catalog: 0,01 USD/tháng Amazon Athena: 0,74 USD/tháng (10 truy vấn/ngày, 0,5 GB mỗi truy vấn) Amazon Redshift Serverless: 7,32 USD/tháng (4 RPU, ~0,16 giờ/ngày) Tổng: 15,38 USD/tháng ≈ 184,56 USD / 12 tháng\nChi phí demo Amazon QuickSight: 9–12 USD/tháng (1 Author, không dùng SPICE, xóa khi không dùng) 7. Đánh giá rủi ro Tính sẵn sàng cục bộ: Máy chủ local mất kết nối sẽ làm gián đoạn việc thu thập.\nGiảm thiểu: Cấu hình cơ chế \u0026ldquo;Catch-up\u0026rdquo; trong Airflow để tự động chạy lại task bị lỡ. API Rate Limit: Vượt ngưỡng 100 QPM có thể bị Reddit chặn.\nGiảm thiểu: Giới hạn 5-10 subreddit và sử dụng hàng chờ Celery để điều phối lưu lượng request. Schema Drift: Cấu trúc dữ liệu Reddit thay đổi bất ngờ làm hỏng script ETL.\nGiảm thiểu: Sử dụng Glue Crawler để tự phát hiện thay đổi schema tự động. 8. Kết quả kỳ vọng Hệ thống thu thập dữ liệu Reddit tự động 100%, không cần can thiệp thủ công. Tối ưu hóa chi phí: Tận dụng tài nguyên sẵn có để giữ mức phí AWS dưới $30/tháng. Khả năng mở rộng: Dễ dàng thêm subreddit chỉ bằng cách cập nhật cấu hình Airflow local. Dashboard trực quan cung cấp cái nhìn chi tiết về xu hướng thảo luận mạng xã hội theo thời gian thực. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/5.2-prerequiste/","title":"Các bước chuẩn bị","tags":[],"description":"","content":"IAM permissions Gắn IAM permission policy sau vào tài khoản aws user của bạn để triển khai và dọn dẹp tài nguyên trong workshop này.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;events:*\u0026#34;, \u0026#34;schemas:*\u0026#34;, \u0026#34;scheduler:*\u0026#34;, \u0026#34;pipes:*\u0026#34;, \u0026#34;redshift:*\u0026#34;, \u0026#34;redshift-serverless:*\u0026#34;, \u0026#34;redshift-data:*\u0026#34;, \u0026#34;sqlworkbench:*\u0026#34;, \u0026#34;athena:*\u0026#34;, \u0026#34;glue:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;s3-object-lambda:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;logs:*\u0026#34;, \u0026#34;observabilityadmin:*\u0026#34;, \u0026#34;ec2:*\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:ListAliases\u0026#34;, \u0026#34;secretsmanager:*\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:ListRolePolicies\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ChangePassword\u0026#34;, \u0026#34;iam:GetAccountPasswordPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;redshift:CreateClusterUser\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:redshift:*:*:dbuser:*/redshift_data_api_user\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iam:CreateServiceLinkedRole\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:iam::*:role/aws-service-role/redshift.amazonaws.com/AWSServiceRoleForRedshift\u0026#34;, \u0026#34;arn:aws:iam::*:role/aws-service-role/redshift-data.amazonaws.com/AWSServiceRoleForRedshift\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;iam:AWSServiceName\u0026#34;: [ \u0026#34;redshift.amazonaws.com\u0026#34;, \u0026#34;redshift-data.amazonaws.com\u0026#34; ] } } } ] } Tạo Reddit API application để Apache Airflow crawl dữ liệu Tạo ứng dụng loại Script App được tạo trên Reddit Developer Portal để được cấp Client ID và Client Secret. Các thông tin này được dùng để xác thực khi Airflow gọi Reddit API (tài khoản tạo cần được xác thực an toàn \u0026gt; 10 ngày)\nSau đó sẽ được khai báo tại config.conf:\n[api-keys] reddit_secret_key = [your secret key] reddit_client_key = [your client key] Khởi tạo S3 Data Lake Structure Trong lab này, chúng ta sẽ dùng N.Virginia region (us-east-1).\nTrong dự án, Data Lake được xây dựng trên Amazon S3 để làm tầng lưu trữ trung tâm cho toàn bộ dữ liệu Reddit, phục vụ cho cả xử lý batch, phân tích lịch sử và truy vấn BI.\nData Lake được thiết kế theo từng tầng giúp phân tách rõ ràng từng giai đoạn của vòng đời dữ liệu:\nKhai báo các biến tại config.conf Mục đích: Sử dung chung và đảm bảo bảo mật [database] database_host = localhost database_name = airflow_reddit database_port = 5432 database_username = postgres database_password = postgres [file_paths] input_path = /opt/airflow/data/input output_path = /opt/airflow/data/output [api-keys] reddit_secret_key = [your secret key] reddit_client_key = [your client key] [aws] aws_access_key_id = [your-aws-key] aws_secret_access_key_id = [your-secret-key] aws_region = us-east-1 aws_bucket_name = amzn-s3-reddit-airflow-project [etl_settings] batch_size = 100 error_handling = abort log_level = info Sau đó gán tên các biến tại constants.py import configparser import os parse = configparser.ConfigParser() parse.read(os.path.join(os.path.dirname(__file__), \u0026#39;../config/config.conf\u0026#39;)) SECRET = parse.get(\u0026#39;api-keys\u0026#39;, \u0026#39;reddit_secret_key\u0026#39;) CLIENT_ID = parse.get(\u0026#39;api-keys\u0026#39;, \u0026#39;reddit_client_key\u0026#39;) DATABASE_HOST = parse.get(\u0026#39;database\u0026#39;, \u0026#39;database_host\u0026#39;) DATABASE_NAME = parse.get(\u0026#39;database\u0026#39;, \u0026#39;database_name\u0026#39;) DATABASE_PORT = parse.get(\u0026#39;database\u0026#39;, \u0026#39;database_port\u0026#39;) DATABASE_USER = parse.get(\u0026#39;database\u0026#39;, \u0026#39;database_username\u0026#39;) DATABASE_PASSWORD = parse.get(\u0026#39;database\u0026#39;, \u0026#39;database_password\u0026#39;) #aws AWS_ACCESS_KEY_ID = parse.get(\u0026#39;aws\u0026#39;,\u0026#39;aws_access_key_id\u0026#39;) AWS_SECRET_KEY = parse.get(\u0026#39;aws\u0026#39;, \u0026#39;aws_secret_access_key_id\u0026#39;) AWS_REGION = parse.get(\u0026#39;aws\u0026#39;, \u0026#39;aws_region\u0026#39;) AWS_BUCKET_NAME = parse.get(\u0026#39;aws\u0026#39;,\u0026#39;aws_bucket_name\u0026#39;) INPUT_PATH = parse.get(\u0026#39;file_paths\u0026#39;, \u0026#39;input_path\u0026#39;) OUTPUT_PATH = parse.get(\u0026#39;file_paths\u0026#39;, \u0026#39;output_path\u0026#39;) POST_FIELDS = ( \u0026#39;id\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;score\u0026#39;, \u0026#39;num_comments\u0026#39;, \u0026#39;author\u0026#39;, \u0026#39;created_utc\u0026#39;, \u0026#39;url\u0026#39;, \u0026#39;over_18\u0026#39;, \u0026#39;edited\u0026#39;, \u0026#39;spoiler\u0026#39;, \u0026#39;stickied\u0026#39; ) Thiết lập Amazon Redshift Serverless Amazon Redshift Serverless làm kho dữ liệu trung tâm cho hệ thống phân tích Reddit.\nViệc sử dụng Redshift Serverless giúp loại bỏ nhu cầu quản lý cluster vật lý, đồng thời đảm bảo hệ thống có khả năng tự động mở rộng tài nguyên theo khối lượng truy vấn.\nQuá trình thiết lập gồm các bước chính:\nTạo Namespace để quản lý metadata và database và Workgroup để cung cấp tài nguyên compute để xử lý truy vấn\nNamespace: reddit-namespace\nWorkgroup: reddit-workgroup Sau khi hoàn tất, Redshift Serverless sẵn sàng đóng vai trò là Data Warehouse trung tâm, kết nối trực tiếp với S3 Data Lake và Glue Data Catalog để phục vụ các bước xử lý và phân tích tiếp theo. Khởi tạo Database phân tích\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/5.4-data-catalog/5.4.2-data-cataloging-pipeline/","title":"Pipeline quản lý danh mục dữ liệu","tags":[],"description":"","content":"1. Tạo Glue Database Glue Database dùng để gom nhóm các bảng Redditvà quản lý metadata tập trung 2. Tạo Glue Job Dữ liệu đã nằm trong S3 (tầng Raw) cần được chuẩn hóa, làm sạch và tối ưu dữ liệu trước khi đưa vào truy vấn. Job được tạo với Source từ tầng raw (s3://amzn-s3-reddit-airflow-project/raw/ \u0026lt;file cần làm sạch\u0026gt;) và đích là tầng S3 tầng transformed (s3://amzn-s3-reddit-airflow-project/transformed/):\nViết thêm scripts để chuẩn hóa dữ liệu:\nDữ liệu được đọc từ S3 dưới dạng DynamicFrame, sau đó chuyển sang Spark DataFrame để thực hiện các phép biến đổi. from awsglue import DynamicFrame from pyspark.sql.functions import concat_ws Ba thuộc tính edited, spoiler và stickied được kết hợp thành một cột duy nhất (ESS_updated) nhằm đơn giản hóa schema và tạo ra một đặc trưng tổng hợp cho phân tích. Sau khi xử lý, dữ liệu được chuyển lại thành DynamicFrame. Cuối cùng, dữ liệu đã được chuẩn hóa được ghi vào S3 (Transformed Zone), sẵn sàng cho việc truy vấn và phân tích bằng Athena và Redshift. #convert DynamicFrame to DataFrame df = AmazonS3_node1766723555757.toDF() #concatenate three columns into a single columns df_combined = df.withColumn(\u0026#39;ESS_updated\u0026#39;, concat_ws(\u0026#39;-\u0026#39;, df[\u0026#39;edited\u0026#39;], df[\u0026#39;spoiler\u0026#39;], df[\u0026#39;stickied\u0026#39;])) df_combined = df_combined.drop(\u0026#39;edited\u0026#39;,\u0026#39;spoiler\u0026#39;,\u0026#39;stickied\u0026#39;) #convert back to DynamicFrame S3bucket_node_combined = DynamicFrame.fromDF(df_combined,glueContext,\u0026#39;S3bucket_node_combined\u0026#39;) # Script generated for node Amazon S3 EvaluateDataQuality().process_rows(frame=S3bucket_node_combined, ruleset=DEFAULT_DATA_QUALITY_RULESET, publishing_options={\u0026#34;dataQualityEvaluationContext\u0026#34;: \u0026#34;EvaluateDataQuality_node1766723551899\u0026#34;, \u0026#34;enableDataQualityResultsPublishing\u0026#34;: True}, additional_options={\u0026#34;dataQualityResultsPublishing.strategy\u0026#34;: \u0026#34;BEST_EFFORT\u0026#34;, \u0026#34;observations.scope\u0026#34;: \u0026#34;ALL\u0026#34;}) AmazonS3_node1766723556784 = glueContext.write_dynamic_frame.from_options(frame=S3bucket_node_combined, connection_type=\u0026#34;s3\u0026#34;, format=\u0026#34;csv\u0026#34;, connection_options={\u0026#34;path\u0026#34;: \u0026#34;s3://amzn-s3-reddit-airflow-project/transformed/\u0026#34;, \u0026#34;partitionKeys\u0026#34;: []}, transformation_ctx=\u0026#34;AmazonS3_node1766723556784\u0026#34;) job.commit()\tChọn IAM Role phù hợp gồm các policy AmazonS3ReadOnlyAccess, AWSGlueServiceRole, và customer inline: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::amzn-s3-reddit-airflow-project\u0026#34;, \u0026#34;arn:aws:s3:::amzn-s3-reddit-airflow-project/*\u0026#34; ] } ] } Sau khi chạy Job thành công, dữ liệu chuẩn hóa sẽ được chuyển sang tầng transform: 2. Tạo Glue Crawler Crawler được cấu hình: Name: reddit_crawler Data source: Amazon S3 Path: s3://amzn-s3-reddit-airflow-project/transformed/ (lấy từ tầng đã được chuẩn hóa) IAM role: Tạo mới role với tên AWSGlueServiceRole-Reddit-glue-role Tên Database lưu trữ: reddit_db Kết quả: Crawler sẽ duyệt toàn bộ file dữ liệu đã được chuẩn hóa, phát hiện schema tự động và lưu trữ vào cơ sở dữ liệu Chạy crawler: Glue đọc JSON và sinh bảng, sau đó cập nhật Glue Catalog "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/5.3-ingestion/5.3.2-ingestion-pipeline/","title":"Quá trình thu thập dữ liệu","tags":[],"description":"","content":"Kết nối Reddit API Hệ thống sử dụng Reddit API thông qua ứng dụng Developer loại script để thu thập bài viết và bình luận từ các subreddit như dataengineering.\nAirflow sử dụng thư viện PRAW để: Xác thực OAuth, truy xuất dữ liệu và tuân thủ giới hạn requests\ndef connect_reddit(client_id, client_secret, user_agent) -\u0026gt; Reddit: try: reddit = praw.Reddit(client_id = client_id, client_secret = client_secret, user_agent = user_agent) print(\u0026#34;Connect to Reddit\u0026#34;) return reddit except Exception as e: print(e) sys.exit(1) DAG Reddit ETL trong Airflow Airflow được sử dụng để điều phối toàn bộ quá trình thu thập và lưu trữ dữ liệu Reddit.\nCần khai báo DAG và 1 taskchạy theo lịch:\ndag = DAG( dag_id=\u0026#39;etl_reddit_pipeline\u0026#39;, default_args=default_args, schedule_interval=\u0026#39;@daily\u0026#39;, catchup=False, tags=[\u0026#39;reddit\u0026#39;, \u0026#39;etl\u0026#39;, \u0026#39;pipeline\u0026#39;] ) Chạy docker kiểm tra DAG chạy thành công không ( cần chạy dịch vụ airflow-init trước để khởi tạo database và tài khoản airflow) DAG chạy thành công Xử lý dữ liệu Reddit Các trường dữ liệu bao gồm: id, title, score, num_comments, author, created_utc, url, over_18, edited , spoiler, stickied Hàm extract_posts được sử dụng để truy vấn Reddit API thông qua thư viện PRAW và lấy danh sách các bài viết từ một subreddit theo bộ lọc thời gian. def extract_posts (reddit_instance: Reddit, subreddit:str, time_filter:str, limit = None): subreddit = reddit_instance.subreddit(subreddit) posts = subreddit.top(time_filter = time_filter, limit=limit) post_lists = [] for post in posts: post_dict = vars(post) post = {key: post_dict[key] for key in POST_FIELDS} post_lists.append(post) return post_lists Mỗi bài viết được chuyển thành dạng dictionary và chỉ giữ lại các trường cần thiết nhằm giảm kích thước dữ liệu và tối ưu cho quá trình xử lý. Hàm transform_data thực hiện chuẩn hóa dữ liệu trước khi lưu vào Data Lake. def transform_data (post_df: pd.DataFrame): post_df[\u0026#39;created_utc\u0026#39;] = pd.to_datetime(post_df.get(\u0026#39;created_utc\u0026#39;, None),unit=\u0026#39;s\u0026#39;,errors=\u0026#39;coerce\u0026#39;) post_df[\u0026#39;over_18\u0026#39;] = post_df.get(\u0026#39;over_18\u0026#39;, False) post_df[\u0026#39;over_18\u0026#39;] = post_df[\u0026#39;over_18\u0026#39;].fillna(False).astype(bool) post_df[\u0026#39;author\u0026#39;] = post_df[\u0026#39;author\u0026#39;].astype(str) edited_mode = post_df[\u0026#39;edited\u0026#39;].mode() post_df[\u0026#39;edited\u0026#39;] = np.where(post_df[\u0026#39;edited\u0026#39;].isin([True, False]), post_df[\u0026#39;edited\u0026#39;],edited_mode).astype(bool) post_df[\u0026#39;num_comments\u0026#39;] = post_df[\u0026#39;num_comments\u0026#39;].astype(int) post_df[\u0026#39;score\u0026#39;] = post_df[\u0026#39;score\u0026#39;].astype(int) post_df[\u0026#39;title\u0026#39;] = post_df[\u0026#39;title\u0026#39;].astype(str) return post_df Tóm tắt Chúc mừng bạn đã hoàn thành thu thập dữ liệu. Trong phần này, dữ liệu đã được thu thập và chuẩn hóa trước khi đưa vào Data Lake (dữ liệu thô sang dạng bảng có cấu trúc, sẵn sàng cho việc lưu trữ trên Amazon S3 và phân tích bằng Athena và Redshift trong các bước tiếp theo của pipeline). "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/3-blogstranslated/","title":"Các bài blogs đã dịch","tags":[],"description":"","content":"Blog 1 - Build and manage your modern data stack using dbt and AWS Glue through dbt-glue, the new \u0026ldquo;trusted\u0026rdquo; dbt adapter Bài blog trình bày cách adapter dbt-glue hỗ trợ tối ưu hóa workflow và cách xây dựng, quản lý nền tảng dữ liệu hiện đại bằng cách kết hợp dbt với AWS Glue. Nội dung giúp làm rõ quy trình end-to-end để triển khai một modern data stack dựa trên dbt và adapter dbt-glue.\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/5.3-ingestion/","title":"Thu thập dữ liệu","tags":[],"description":"","content":"Sử dụng Airflow Trong giai đoạn thu thập dữ liệu, hệ thống sử dụng Apache Airflow làm nền tảng điều phối để tự động hóa quá trình lấy dữ liệu từ Reddit API và đưa vào hệ thống Data Lake.\nToàn bộ môi trường Airflow được triển khai trong Docker, bao gồm các thành phần chính: Webserver, Scheduler, Workers, PostgreSQL và Celery. Nội dung Cấu hình Docker Quy trình thu thập dữ liệu "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/4-eventparticipated/","title":"Các events đã tham gia","tags":[],"description":"","content":"Trước khi thực tập, em đã tham gia event kick off, đây là một trải nghiệm đáng nhớ với những chia sẻ từ các anh chị đi trước, các dự án hay và bổ ích, giúp em có thêm động lực khám phá những kiến thức mới mẻ tại AWS.\nEvent 1 Tên sự kiện: AWS FIRST CLOUD JOURNEY COMMUNITY DAY 2025\nThời gian: 09:00 ngày 30/08/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/5.4-data-catalog/","title":"Nạp dữ liệu vào Data Lake và Quản lý Metadata","tags":[],"description":"","content":"Tổng quan Sau khi dữ liệu được thu thập bởi Airflow, hệ thống nạp dữ liệu vào Data Lake trên Amazon S3 và sử dụng AWS Glue để tự động tạo metadata. Amazon S3 đóng vai trò lưu trữ trung tâm cho dữ liệu Reddit. Glue Crawler quét dữ liệu và xây dựng các bảng trong Glue Data Catalog. Nhờ đó, dữ liệu từ dạng file thô được chuyển thành các bảng có thể truy vấn bằng Athena và Redshift. Quy trình này giúp tự động hóa việc nạp dữ liệu, quản lý schema và tạo nền tảng cho các bước phân tích tiếp theo. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/5.5-analytics/","title":"Truy vấn dữ liệu và trực quan hóa","tags":[],"description":"","content":"1. Truy vấn dữ liệu với Amazon Athena Sau khi Glue Crawler đã tạo bảng trong Glue Data Catalog, dữ liệu trong S3 (Transformed Zone) có thể được truy vấn trực tiếp bằng Amazon Athena mà không cần di chuyển dữ liệu.\nAthena sử dụng metadata từ Glue Catalog để hiểu: Schema, Data type, Location của file trong S3\nCấu hình Athena\nChọn Database: reddit_db Chọn bảng: transformed Chọn Query settings -\u0026gt; gắn kết quả truy vấn được lưu tại: s3://amzn-s3-reddit-airflow-project/athena-script/ Ví dụ truy vấn:\nSELECT author, COUNT(*) AS total_posts, AVG(score) AS avg_score, SUM(num_comments) AS total_comments FROM \u0026#34;AwsDataCatalog\u0026#34;.\u0026#34;reddit_db\u0026#34;.\u0026#34;transformed\u0026#34; GROUP BY author ORDER BY total_posts DESC; Đạt kết quả như sau: Ở phần đầu phân tích, Athena được sử dụng để phân tích nhanh, kiểm tra dữ liệu và Validate chất lượng ETL 2. Nạp dữ liệu từ S3 vào Amazon Redshift Mặc dù Athena phù hợp cho truy vấn ad-hoc, nhưng để BI DashboardDashboard, Join lớnlớn, Performance cao thì dữ liệu cần được đưa vào Amazon Redshift (Data Warehouse).\nVới các cấu hình đã được thiết lập sẵn ở phần chuẩn bị, bao gồm tạo namespace và workgroup, ta cần chọn query data - \u0026gt; Query in query Editor v2: Sau đó connect tới workgroup đã tạo, sử dụng Federated user để không cần dùng password cho database: Tiếp đó load dữ liệu từ S3 sang Data Warehouse: Chọn tiếp schema public, đặt tên bảng trong kho dữ liệu và chọn IAM role mặc định: Kho dữ liệu sau khi được load thành công: 3. Định hướng trực quan hóa và phân tích nâng cao Mặc dù ở giai đoạn hiện tại khối lượng dữ liệu Reddit chưa đủ lớn để triển khai hiệu quả các hệ thống Business Intelligence (BI), kiến trúc nền tảng đã được thiết kế sẵn sàng cho việc mở rộng trong tương lai.\nKhi dữ liệu tiếp tục được thu thập theo thời gian (daily ingestion từ Airflow), kho dữ liệu Amazon Redshift sẽ dần hình thành một historical data warehouse với độ sâu theo thời gian. Tại thời điểm đó, các công cụ BI như Amazon QuickSight có thể được tích hợp trực tiếp vào Redshift để:\nTrực quan hóa xu hướng tăng trưởng bài viết theo subreddit\nPhân tích hành vi tương tác (score, comment, ESS_updated) theo thời gian\nTheo dõi sự thay đổi của chủ đề và mức độ ảnh hưởng\nHỗ trợ ra quyết định dựa trên dữ liệu lịch sử và dữ liệu gần real-time\nNhư vậy, hệ thống hiện tại không chỉ phục vụ cho việc xử lý và truy vấn dữ liệu, mà còn đóng vai trò là một nền tảng phân tích dữ liệu doanh nghiệp hoàn chỉnh, sẵn sàng mở rộng sang lớp trực quan hóa và phân tích nâng cao khi dữ liệu đạt quy mô đủ lớn.\n"},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Tự động hóa pipeline ETL cho dữ liệu Reddit trên AWS Tổng quan Dự án này xây dựng một hệ thống tự động hóa phân tích dữ liệu Reddit trên AWS với các thành phần chính sau:\nThu thập và lưu trữ dữ liệu: Dữ liệu được lấy tự động từ Reddit API bằng Apache Airflow và lưu trữ trên Amazon S3 theo mô hình Data Lake (Raw và Transformed).\nXử lý và quản lý dữ liệu: AWS Glue ETL Jobs được sử dụng để làm sạch và chuyển đổi dữ liệu, trong khi AWS Glue Data Catalog và Crawler quản lý schema và metadata.\nPhân tích và truy vấn: Amazon Athena cho phép truy vấn trực tiếp trên Data Lake, còn Amazon Redshift đóng vai trò Data Warehouse cho các truy vấn phân tích.\nTrực quan hóa: Amazon QuickSight được dùng để xây dựng dashboard và hiển thị insight từ dữ liệu.\nHệ thống giúp dữ liệu Reddit được cập nhật và phân tích hoàn toàn tự động, hỗ trợ theo dõi xu hướng và hành vi người dùng trên nền tảng AWS.\nNội dung Tổng quan về workshop Chuẩn bị Thu thập dữ liệu Data Catalog Phân tích dữ liệu và báo cáo Dọn dẹp tài nguyên "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/6-self-evaluation/","title":"Tự đánh giá","tags":[],"description":"","content":"Trong quá trình thực tập tại Công ty TNHH Amazon Web Services Việt Nam, tôi có cơ hội tham gia vào dự án xây dựng hệ thống xử lý và phân tích dữ liệu trên nền tảng AWS, với dữ liệu được thu thập và xử lý theo mô hình ETL – Data Lake – Data Warehouse.\nThông qua dự án này, tôi đã vận dụng kiến thức học được tại trường để xây dựng một hệ thống dữ liệu thực tế, bao gồm:\nThu thập dữ liệu từ Reddit API bằng Python và Airflow Lưu trữ dữ liệu thô vào Amazon S3 (Data Lake) Xử lý, làm sạch và chuẩn hóa dữ liệu bằng AWS Glue Truy vấn và phân tích dữ liệu bằng Amazon Athena Nạp dữ liệu đã xử lý vào Amazon Redshift (Data Warehouse) để phục vụ báo cáo và phân tích Trực quan hóa dữ liệu bằng Amazon QuickSight Quá trình này giúp tôi hiểu rõ hơn về vòng đời của dữ liệu trong một hệ thống Data Engineering, từ thu thập, lưu trữ, xử lý cho đến phân tích và trực quan hóa. Đồng thời, tôi cũng học được cách làm việc với hạ tầng cloud, phân quyền IAM, logging, giám sát và tối ưu chi phí trên AWS.\nVề thái độ làm việc, tôi luôn cố gắng hoàn thành nhiệm vụ đúng tiến độ, chủ động tìm hiểu tài liệu AWS khi gặp vấn đề kỹ thuật, và thường xuyên trao đổi với mentor cũng như đồng đội để cải thiện chất lượng hệ thống.\nĐể phản ánh khách quan quá trình thực tập, tôi tự đánh giá bản thân theo các tiêu chí sau:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ✅ ☐ ☐ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ✅ ☐ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ✅ ☐ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ☐ ✅ ☐ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ☐ ✅ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ☐ ✅ ☐ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ☐ ✅ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team ✅ ☐ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ✅ ☐ ☐ Cần cải thiện Cải thiện trong việc sắp xếp công việc và sắp xếp thời gian một cách hợp lý Cải thiện khả năng giao tiếp tốt hơn trong giao tiếp hằng ngày và công việc, cũng như trong nhóm Cải thiện trong cách tư duy giải quyết vấn đề "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/5-workshop/5.6-cleanup/","title":"Dọn dẹp tài nguyên","tags":[],"description":"","content":"Dọn dẹp tài nguyên Do hệ thống Data Platform sử dụng nhiều dịch vụ compute (Glue, Redshift), nếu không kiểm soát, chi phí có thể tăng nhanh ngay cả khi không có người dùng.\n1. Glue Crawler Xóa lịch chạy tự động: Chọn dịch vụ AWS Glue → Crawlers → reddit_crawler → Delete 2. Glue Job Xóa job ETL: Glue → Jobs → reddit_glue_jobjob.py → Action → Delete job(s) 3. Glue Logs Xóa các logs chạy Glue: CloudWatch → Log groups → /aws-glue/* → Delete group logs 4. Xóa các S3 bucket Mở bảng điều khiển S3 Chọn bucket chúng ta đã tạo cho lab, bao gồm bucket lưu trữ tầng raw, transformed, và athena, glue. N Nhấp chuột và xác nhận là empty. Nhấp Delete và xác nhận delete. "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/7-feedback/","title":"Chia sẻ, đóng góp ý kiến","tags":[],"description":"","content":"Đánh giá chung 1. Môi trường làm việc\nMôi trường làm việc rất thân thiện và cởi mở. Các thành viên trong FCJ rất năng động và luôn sẵn sàng hỗ trợ khi mình gặp khó khăn, kể cả ngoài giờ làm việc. Không gian làm việc gọn gàng, thoải mái, giúp mình tập trung tốt hơn. Tuy nhiên, mình nghĩ có thể bổ sung thêm một số buổi giao lưu trò chuyện, rủ nhau ăn trưa để có thể gần gũi nhau hơn.\n2. Sự hỗ trợ của mentor / team admin\nMentor sẳn sàng hỗ trợ, giải thích những gì mình chưa hiểu và luôn khuyến khích mình đặt câu hỏi. Team admin hỗ trợ các thủ tục, tài liệu và tạo điều kiện để mình làm việc thuận lợi.\n3. Sự phù hợp giữa công việc và chuyên ngành học\nCông việc mình được giao phù hợp với kiến thức mình đã học ở trường, đồng thời mở rộng thêm những mảng mới mà mình chưa từng được tiếp cận. Nhờ vậy, mình vừa củng cố kiến thức nền tảng, vừa học thêm kỹ năng thực tế.\n4. Cơ hội học hỏi \u0026amp; phát triển kỹ năng\nTrong quá trình thực tập, mình học được nhiều kỹ năng mới như sử dụng công cụ quản lý dự án, kỹ năng làm việc nhóm, và cả cách giao tiếp chuyên nghiệp trong môi trường công ty. Mentor cũng chia sẻ nhiều kinh nghiệm thực tế giúp mình định hướng tốt hơn cho sự nghiệp.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa công ty rất tích cực: mọi người tôn trọng lẫn nhau, làm việc nghiêm túc nhưng vẫn vui vẻ. Khi có dự án gấp, mọi người cùng nhau cố gắng, hỗ trợ không phân biệt vị trí.\n6. Chính sách / phúc lợi cho thực tập sinh\nCông ty tạo điều kiện về thời gian linh hoạt khi cần thiết. Ngoài ra, việc được tham gia các buổi workshop, event giúp mình có thêm nhiều kiến thức hơn.\nMột số câu hỏi khác Điều bạn hài lòng nhất trong thời gian thực tập?\nMọi người rất thân thiện, sẵn sàng hỗ trợ bất cứ lúc nào. Đề xuất \u0026amp; mong muốn Bạn có muốn tiếp tục chương trình này trong tương lai? Có "},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://roses19.github.io/fcj-workshop-roses/vi/tags/","title":"Tags","tags":[],"description":"","content":""}]